{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import functools\n",
    "import jax\n",
    "from IPython.display import HTML, display, clear_output\n",
    "from jax import numpy as jp\n",
    "import numpy as np\n",
    "from typing import Any, Dict, Sequence, Tuple, Union, List\n",
    "\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax import math\n",
    "from brax.base import Base, Motion, Transform\n",
    "from brax.envs.base import Env, PipelineEnv, State\n",
    "from brax.mjx.base import State as MjxState\n",
    "from brax.training.agents.ppo import train as ppo\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.io import html, mjcf, model\n",
    "\n",
    "from etils import epath\n",
    "from flax import struct\n",
    "from matplotlib import pyplot as plt\n",
    "import mediapy as media\n",
    "from ml_collections import config_dict\n",
    "import mujoco\n",
    "from mujoco import mjx\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.training.acme import specs\n",
    "import os\n",
    "\n",
    "import functools\n",
    "import time\n",
    "from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "from absl import logging\n",
    "from brax import base\n",
    "from brax import envs\n",
    "from brax.training import acting\n",
    "from brax.training import gradients\n",
    "from brax.training import pmap\n",
    "from brax.training import types\n",
    "from brax.training.acme import running_statistics\n",
    "from brax.training.acme import specs\n",
    "from brax.training.agents.ppo import losses as ppo_losses\n",
    "from brax.training.agents.ppo import networks as ppo_networks\n",
    "from brax.training.types import Params\n",
    "from brax.training.types import PRNGKey\n",
    "from brax.v1 import envs as envs_v1\n",
    "import flax\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "from brax.training import distribution\n",
    "from brax.training import networks\n",
    "from brax.training import types\n",
    "from brax.training.types import PRNGKey\n",
    "import flax\n",
    "from flax import linen\n",
    "from typing import Any, Callable, Sequence, Tuple\n",
    "import dataclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'XLA_FLAGS' in os.environ:\n",
    "    print(f\"XLA_FLAGS = {os.environ['XLA_FLAGS']}\")\n",
    "else:\n",
    "    print(f\"Environment variable '{'XLA_FLAGS'}' not set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defunct LSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Sequence, Tuple\n",
    "\n",
    "# from brax.training import distribution\n",
    "# from brax.training import networks\n",
    "# from brax.training import types\n",
    "# from brax.training.types import PRNGKey\n",
    "# import flax\n",
    "# from flax import linen\n",
    "# from brax.training.agents.ppo.networks import PPONetworks\n",
    "# from typing import Any, Callable, Sequence, Tuple\n",
    "# import dataclasses\n",
    "\n",
    "\n",
    "# ActivationFn = Callable[[jp.ndarray], jp.ndarray]\n",
    "# Initializer = Callable[..., Any]\n",
    "\n",
    "# @dataclasses.dataclass\n",
    "# class LSTMNetwork(networks.FeedForwardNetwork):\n",
    "#   carry: jp.ndarray\n",
    "\n",
    "# def make_lstm_policy_network(\n",
    "#     param_size: int,\n",
    "#     obs_size: int,\n",
    "#     preprocess_observations_fn: types.PreprocessObservationFn = types\n",
    "#     .identity_observation_preprocessor,\n",
    "#     hidden_layer_sizes: int = 256,\n",
    "#     activation: ActivationFn = linen.relu) -> networks.FeedForwardNetwork:\n",
    "\n",
    "\n",
    "#   policy_module = linen.RNN(linen.LSTMCell(\n",
    "#     features=hidden_layer_sizes,\n",
    "#     activation_fn=activation,\n",
    "#     recurrent_kernel_init=jax.linen.initializers.lecun_uniform()),\n",
    "#     time_major=True,return_carry=True)\n",
    "\n",
    "#   def apply(processor_params, policy_params, obs, carry=None):\n",
    "#     obs = preprocess_observations_fn(obs, processor_params)\n",
    "#     return policy_module.apply(policy_params, obs, initial_carry=carry)\n",
    "\n",
    "#   dummy_obs = jp.zeros((1, obs_size))\n",
    "#   return LSTMNetwork(init=lambda key: policy_module.init(key, dummy_obs), apply=apply, carry=None)\n",
    "\n",
    "# @flax.struct.dataclass\n",
    "# class LSTM_PPONetworks(PPONetworks):\n",
    "#   mem_network: LSTMNetwork\n",
    "\n",
    "# def make_lstm__inference_fn(ppo_networks: LSTM_PPONetworks):\n",
    "#   \"\"\"Creates params and inference function for the PPO agent.\"\"\"\n",
    "\n",
    "#   def make_policy(params, deterministic: bool = False) -> types.Policy:\n",
    "#     mem_network = ppo_networks.mem_network\n",
    "#     policy_network = ppo_networks.policy_network\n",
    "#     parametric_action_distribution = ppo_networks.parametric_action_distribution\n",
    "#     data = [mem_network.carry]\n",
    "\n",
    "#     def policy(observations: types.Observation, key_sample: PRNGKey) -> Tuple[types.Action, types.Extra]:\n",
    "#       startd = len(observations.shape)\n",
    "#       if len(observations.shape) == 1:\n",
    "#         observations = jp.expand_dims(observations, axis=0)\n",
    "#       if len(observations.shape) == 2:\n",
    "#         observations = jp.expand_dims(observations, axis=0)\n",
    "#       carry, hidden = mem_network.apply(params[0],params[1], observations, carry=data[0])\n",
    "#       data[0] = carry\n",
    "#       while len(hidden.shape) > startd:\n",
    "#         hidden = jp.squeeze(hidden,axis=0)\n",
    "#       logits = policy_network.apply(params[0],params[2], hidden)\n",
    "#       if deterministic:\n",
    "#         return ppo_networks.parametric_action_distribution.mode(logits), {}\n",
    "#       raw_actions = parametric_action_distribution.sample_no_postprocessing(\n",
    "#           logits, key_sample)\n",
    "#       log_prob = parametric_action_distribution.log_prob(logits, raw_actions)\n",
    "#       postprocessed_actions = parametric_action_distribution.postprocess(\n",
    "#           raw_actions)\n",
    "#       return postprocessed_actions, {\n",
    "#           'log_prob': log_prob,\n",
    "#           'raw_action': raw_actions,\n",
    "#           'carry': carry,\n",
    "#       }\n",
    "\n",
    "#     return policy\n",
    "\n",
    "#   return make_policy\n",
    "\n",
    "\n",
    "# def make_lstm_ppo_networks(\n",
    "#     observation_size: int,\n",
    "#     action_size: int,\n",
    "#     preprocess_observations_fn: types.PreprocessObservationFn = types\n",
    "#     .identity_observation_preprocessor,\n",
    "#     policy_hidden_layer_sizes: Sequence[int] = (32,) * 4,\n",
    "#     value_hidden_layer_sizes: Sequence[int] = (256,) * 5,\n",
    "#     mem_hidden_layer_sizes: int = 256,\n",
    "#     activation: networks.ActivationFn = linen.swish) -> PPONetworks:\n",
    "#   \"\"\"Make PPO networks with preprocessor.\"\"\"\n",
    "#   parametric_action_distribution = distribution.NormalTanhDistribution(\n",
    "#       event_size=action_size)\n",
    "#   mem_network = make_lstm_policy_network(\n",
    "#       param_size=parametric_action_distribution.param_size,\n",
    "#       preprocess_observations_fn=preprocess_observations_fn,\n",
    "#       obs_size=observation_size,\n",
    "#       hidden_layer_sizes=mem_hidden_layer_sizes,\n",
    "#       activation=activation)\n",
    "#   policy_network = networks.make_policy_network(\n",
    "#       parametric_action_distribution.param_size,\n",
    "#       mem_hidden_layer_sizes,\n",
    "#       hidden_layer_sizes=policy_hidden_layer_sizes,\n",
    "#       activation=activation)\n",
    "#   value_network = networks.make_value_network(\n",
    "#       mem_hidden_layer_sizes,\n",
    "#       hidden_layer_sizes=value_hidden_layer_sizes,\n",
    "#       activation=activation)\n",
    "  \n",
    "\n",
    "#   return LSTM_PPONetworks(\n",
    "#       mem_network=mem_network,\n",
    "#       policy_network=policy_network,\n",
    "#       value_network=value_network,\n",
    "#       parametric_action_distribution=parametric_action_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @flax.struct.dataclass\n",
    "# class LSTM_PPONetworkParams:\n",
    "#   \"\"\"Contains training state for the learner.\"\"\"\n",
    "#   policy: Any\n",
    "#   value: Any\n",
    "#   mem: Any\n",
    "\n",
    "\n",
    "# def compute_gae(truncation: jp.ndarray,\n",
    "#                 termination: jp.ndarray,\n",
    "#                 rewards: jp.ndarray,\n",
    "#                 values: jp.ndarray,\n",
    "#                 bootstrap_value: jp.ndarray,\n",
    "#                 lambda_: float = 1.0,\n",
    "#                 discount: float = 0.99):\n",
    "#   \"\"\"Calculates the Generalized Advantage Estimation (GAE).\n",
    "\n",
    "#   Args:\n",
    "#     truncation: A float32 tensor of shape [T, B] with truncation signal.\n",
    "#     termination: A float32 tensor of shape [T, B] with termination signal.\n",
    "#     rewards: A float32 tensor of shape [T, B] containing rewards generated by\n",
    "#       following the behaviour policy.\n",
    "#     values: A float32 tensor of shape [T, B] with the value function estimates\n",
    "#       wrt. the target policy.\n",
    "#     bootstrap_value: A float32 of shape [B] with the value function estimate at\n",
    "#       time T.\n",
    "#     lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to\n",
    "#       lambda_=1.\n",
    "#     discount: TD discount.\n",
    "\n",
    "#   Returns:\n",
    "#     A float32 tensor of shape [T, B]. Can be used as target to\n",
    "#       train a baseline (V(x_t) - vs_t)^2.\n",
    "#     A float32 tensor of shape [T, B] of advantages.\n",
    "#   \"\"\"\n",
    "\n",
    "#   truncation_mask = 1 - truncation\n",
    "#   # Append bootstrapped value to get [v1, ..., v_t+1]\n",
    "#   values_t_plus_1 = jp.concatenate(\n",
    "#       [values[1:], jp.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "#   deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values\n",
    "#   deltas *= truncation_mask\n",
    "\n",
    "#   acc = jp.zeros_like(bootstrap_value)\n",
    "#   vs_minus_v_xs = []\n",
    "\n",
    "#   def compute_vs_minus_v_xs(carry, target_t):\n",
    "#     lambda_, acc = carry\n",
    "#     truncation_mask, delta, termination = target_t\n",
    "#     acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc\n",
    "#     return (lambda_, acc), (acc)\n",
    "\n",
    "#   (_, _), (vs_minus_v_xs) = jax.lax.scan(\n",
    "#       compute_vs_minus_v_xs, (lambda_, acc),\n",
    "#       (truncation_mask, deltas, termination),\n",
    "#       length=int(truncation_mask.shape[0]),\n",
    "#       reverse=True)\n",
    "#   # Add V(x_s) to get v_s.\n",
    "#   vs = jp.add(vs_minus_v_xs, values)\n",
    "\n",
    "#   vs_t_plus_1 = jp.concatenate(\n",
    "#       [vs[1:], jp.expand_dims(bootstrap_value, 0)], axis=0)\n",
    "#   advantages = (rewards + discount *\n",
    "#                 (1 - termination) * vs_t_plus_1 - values) * truncation_mask\n",
    "#   return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)\n",
    "\n",
    "\n",
    "# def compute_lstm_ppo_loss(\n",
    "#     params: LSTM_PPONetworkParams,\n",
    "#     normalizer_params: Any,\n",
    "#     data: types.Transition,\n",
    "#     rng: jp.ndarray,\n",
    "#     ppo_network: LSTM_PPONetworks,\n",
    "#     entropy_cost: float = 1e-4,\n",
    "#     discounting: float = 0.9,\n",
    "#     reward_scaling: float = 1.0,\n",
    "#     gae_lambda: float = 0.95,\n",
    "#     clipping_epsilon: float = 0.3,\n",
    "#     normalize_advantage: bool = True) -> Tuple[jp.ndarray, types.Metrics]:\n",
    "#   \"\"\"Computes PPO loss.\n",
    "\n",
    "#   Args:\n",
    "#     params: Network parameters,\n",
    "#     normalizer_params: Parameters of the normalizer.\n",
    "#     data: Transition that with leading dimension [B, T]. extra fields required\n",
    "#       are ['state_extras']['truncation'] ['policy_extras']['raw_action']\n",
    "#         ['policy_extras']['log_prob']\n",
    "#     rng: Random key\n",
    "#     ppo_network: PPO networks.\n",
    "#     entropy_cost: entropy cost.\n",
    "#     discounting: discounting,\n",
    "#     reward_scaling: reward multiplier.\n",
    "#     gae_lambda: General advantage estimation lambda.\n",
    "#     clipping_epsilon: Policy loss clipping epsilon\n",
    "#     normalize_advantage: whether to normalize advantage estimate\n",
    "\n",
    "#   Returns:\n",
    "#     A tuple (loss, metrics)\n",
    "#   \"\"\"\n",
    "#   parametric_action_distribution = ppo_network.parametric_action_distribution\n",
    "#   policy_apply = ppo_network.policy_network.apply\n",
    "#   value_apply = ppo_network.value_network.apply\n",
    "#   mem_apply = ppo_network.mem_network.apply\n",
    "  \n",
    "\n",
    "#   # Put the time dimension first.\n",
    "#   data = jax.tree_util.tree_map(lambda x: jp.swapaxes(x, 0, 1), data)\n",
    "#   carry, hidden = mem_apply(normalizer_params, params.mem, data.observation)\n",
    "#   next_carry, hidden_next = mem_apply(normalizer_params, params.mem, jp.expand_dims(data.next_observation[-1], axis=0), carry=carry)\n",
    "#   hidden_next = jp.squeeze(hidden_next,axis=0)\n",
    "#   policy_logits = policy_apply(normalizer_params, params.policy,\n",
    "#                                hidden)\n",
    "\n",
    "#   baseline = value_apply(normalizer_params, params.value, hidden)\n",
    "\n",
    "#   bootstrap_value = value_apply(normalizer_params, params.value, hidden_next)\n",
    "\n",
    "#   rewards = data.reward * reward_scaling\n",
    "#   truncation = data.extras['state_extras']['truncation']\n",
    "#   termination = (1 - data.discount) * (1 - truncation)\n",
    "\n",
    "#   target_action_log_probs = parametric_action_distribution.log_prob(\n",
    "#       policy_logits, data.extras['policy_extras']['raw_action'])\n",
    "#   behaviour_action_log_probs = data.extras['policy_extras']['log_prob']\n",
    "\n",
    "#   vs, advantages = compute_gae(\n",
    "#       truncation=truncation,\n",
    "#       termination=termination,\n",
    "#       rewards=rewards,\n",
    "#       values=baseline,\n",
    "#       bootstrap_value=bootstrap_value,\n",
    "#       lambda_=gae_lambda,\n",
    "#       discount=discounting)\n",
    "#   if normalize_advantage:\n",
    "#     advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "#   rho_s = jp.exp(target_action_log_probs - behaviour_action_log_probs)\n",
    "\n",
    "#   surrogate_loss1 = rho_s * advantages\n",
    "#   surrogate_loss2 = jp.clip(rho_s, 1 - clipping_epsilon,\n",
    "#                              1 + clipping_epsilon) * advantages\n",
    "\n",
    "#   policy_loss = -jp.mean(jp.minimum(surrogate_loss1, surrogate_loss2))\n",
    "\n",
    "#   # Value function loss\n",
    "#   v_error = vs - baseline\n",
    "#   v_loss = jp.mean(v_error * v_error) * 0.5 * 0.5\n",
    "\n",
    "#   # Entropy reward\n",
    "#   entropy = jp.mean(parametric_action_distribution.entropy(policy_logits, rng))\n",
    "#   entropy_loss = entropy_cost * -entropy\n",
    "\n",
    "#   total_loss = policy_loss + v_loss + entropy_loss\n",
    "#   return total_loss, {\n",
    "#       'total_loss': total_loss,\n",
    "#       'policy_loss': policy_loss,\n",
    "#       'v_loss': v_loss,\n",
    "#       'entropy_loss': entropy_loss\n",
    "#   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functools\n",
    "# import time\n",
    "# from typing import Callable, Optional, Tuple, Union\n",
    "\n",
    "# from absl import logging\n",
    "# from brax import base\n",
    "# from brax import envs\n",
    "# from brax.training import acting\n",
    "# from brax.training import gradients\n",
    "# from brax.training import pmap\n",
    "# from brax.training import types\n",
    "# from brax.training.acme import running_statistics\n",
    "# from brax.training.acme import specs\n",
    "# from brax.training.agents.ppo import losses as ppo_losses\n",
    "# from brax.training.agents.ppo import networks as ppo_networks\n",
    "# from brax.training.types import Params\n",
    "# from brax.training.types import PRNGKey\n",
    "# from brax.v1 import envs as envs_v1\n",
    "# import flax\n",
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import numpy as np\n",
    "# import optax\n",
    "\n",
    "\n",
    "# InferenceParams = Tuple[running_statistics.NestedMeanStd, Params]\n",
    "# Metrics = types.Metrics\n",
    "\n",
    "# _PMAP_AXIS_NAME = 'i'\n",
    "\n",
    "\n",
    "# @flax.struct.dataclass\n",
    "# class TrainingState:\n",
    "#   \"\"\"Contains training state for the learner.\"\"\"\n",
    "#   optimizer_state: optax.OptState\n",
    "#   params: LSTM_PPONetworkParams\n",
    "#   normalizer_params: running_statistics.RunningStatisticsState\n",
    "#   env_steps: jnp.ndarray\n",
    "\n",
    "\n",
    "# def _unpmap(v):\n",
    "#   return jax.tree_util.tree_map(lambda x: x[0], v)\n",
    "\n",
    "\n",
    "# def _strip_weak_type(tree):\n",
    "#   # brax user code is sometimes ambiguous about weak_type.  in order to\n",
    "#   # avoid extra jit recompilations we strip all weak types from user input\n",
    "#   def f(leaf):\n",
    "#     leaf = jnp.asarray(leaf)\n",
    "#     return leaf.astype(leaf.dtype)\n",
    "#   return jax.tree_util.tree_map(f, tree)\n",
    "\n",
    "# def train(\n",
    "#     environment: Union[envs_v1.Env, envs.Env],\n",
    "#     num_timesteps: int,\n",
    "#     episode_length: int,\n",
    "#     action_repeat: int = 1,\n",
    "#     num_envs: int = 1,\n",
    "#     max_devices_per_host: Optional[int] = None,\n",
    "#     num_eval_envs: int = 128,\n",
    "#     learning_rate: float = 1e-4,\n",
    "#     entropy_cost: float = 1e-4,\n",
    "#     discounting: float = 0.9,\n",
    "#     seed: int = 0,\n",
    "#     unroll_length: int = 10,\n",
    "#     batch_size: int = 32,\n",
    "#     num_minibatches: int = 16,\n",
    "#     num_updates_per_batch: int = 2,\n",
    "#     num_evals: int = 1,\n",
    "#     num_resets_per_eval: int = 0,\n",
    "#     normalize_observations: bool = False,\n",
    "#     reward_scaling: float = 1.0,\n",
    "#     clipping_epsilon: float = 0.3,\n",
    "#     gae_lambda: float = 0.95,\n",
    "#     deterministic_eval: bool = False,\n",
    "#     network_factory: types.NetworkFactory[\n",
    "#         LSTM_PPONetworks\n",
    "#     ] = make_lstm_ppo_networks,\n",
    "#     progress_fn: Callable[[int, Metrics], None] = lambda *args: None,\n",
    "#     normalize_advantage: bool = True,\n",
    "#     eval_env: Optional[envs.Env] = None,\n",
    "#     policy_params_fn: Callable[..., None] = lambda *args: None,\n",
    "#     randomization_fn: Optional[\n",
    "#         Callable[[base.System, jnp.ndarray], Tuple[base.System, base.System]]\n",
    "#     ] = None,\n",
    "# ):\n",
    "#   \"\"\"PPO training.\n",
    "\n",
    "#   Args:\n",
    "#     environment: the environment to train\n",
    "#     num_timesteps: the total number of environment steps to use during training\n",
    "#     episode_length: the length of an environment episode\n",
    "#     action_repeat: the number of timesteps to repeat an action\n",
    "#     num_envs: the number of parallel environments to use for rollouts\n",
    "#       NOTE: `num_envs` must be divisible by the total number of chips since each\n",
    "#         chip gets `num_envs // total_number_of_chips` environments to roll out\n",
    "#       NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since\n",
    "#         data generated by `num_envs` parallel envs gets used for gradient\n",
    "#         updates over `num_minibatches` of data, where each minibatch has a\n",
    "#         leading dimension of `batch_size`\n",
    "#     max_devices_per_host: maximum number of chips to use per host process\n",
    "#     num_eval_envs: the number of envs to use for evluation. Each env will run 1\n",
    "#       episode, and all envs run in parallel during eval.\n",
    "#     learning_rate: learning rate for ppo loss\n",
    "#     entropy_cost: entropy reward for ppo loss, higher values increase entropy\n",
    "#       of the policy\n",
    "#     discounting: discounting rate\n",
    "#     seed: random seed\n",
    "#     unroll_length: the number of timesteps to unroll in each environment. The\n",
    "#       PPO loss is computed over `unroll_length` timesteps\n",
    "#     batch_size: the batch size for each minibatch SGD step\n",
    "#     num_minibatches: the number of times to run the SGD step, each with a\n",
    "#       different minibatch with leading dimension of `batch_size`\n",
    "#     num_updates_per_batch: the number of times to run the gradient update over\n",
    "#       all minibatches before doing a new environment rollout\n",
    "#     num_evals: the number of evals to run during the entire training run.\n",
    "#       Increasing the number of evals increases total training time\n",
    "#     num_resets_per_eval: the number of environment resets to run between each\n",
    "#       eval. The environment resets occur on the host\n",
    "#     normalize_observations: whether to normalize observations\n",
    "#     reward_scaling: float scaling for reward\n",
    "#     clipping_epsilon: clipping epsilon for PPO loss\n",
    "#     gae_lambda: General advantage estimation lambda\n",
    "#     deterministic_eval: whether to run the eval with a deterministic policy\n",
    "#     network_factory: function that generates networks for policy and value\n",
    "#       functions\n",
    "#     progress_fn: a user-defined callback function for reporting/plotting metrics\n",
    "#     normalize_advantage: whether to normalize advantage estimate\n",
    "#     eval_env: an optional environment for eval only, defaults to `environment`\n",
    "#     policy_params_fn: a user-defined callback function that can be used for\n",
    "#       saving policy checkpoints\n",
    "#     randomization_fn: a user-defined callback function that generates randomized\n",
    "#       environments\n",
    "\n",
    "#   Returns:\n",
    "#     Tuple of (make_policy function, network params, metrics)\n",
    "#   \"\"\"\n",
    "#   assert batch_size * num_minibatches % num_envs == 0\n",
    "#   xt = time.time()\n",
    "\n",
    "#   process_count = jax.process_count()\n",
    "#   process_id = jax.process_index()\n",
    "#   local_device_count = jax.local_device_count()\n",
    "#   local_devices_to_use = local_device_count\n",
    "#   if max_devices_per_host:\n",
    "#     local_devices_to_use = min(local_devices_to_use, max_devices_per_host)\n",
    "#   logging.info(\n",
    "#       'Device count: %d, process count: %d (id %d), local device count: %d, '\n",
    "#       'devices to be used count: %d', jax.device_count(), process_count,\n",
    "#       process_id, local_device_count, local_devices_to_use)\n",
    "#   device_count = local_devices_to_use * process_count\n",
    "\n",
    "#   # The number of environment steps executed for every training step.\n",
    "#   env_step_per_training_step = (\n",
    "#       batch_size * unroll_length * num_minibatches * action_repeat)\n",
    "#   num_evals_after_init = max(num_evals - 1, 1)\n",
    "#   # The number of training_step calls per training_epoch call.\n",
    "#   # equals to ceil(num_timesteps / (num_evals * env_step_per_training_step *\n",
    "#   #                                 num_resets_per_eval))\n",
    "#   num_training_steps_per_epoch = np.ceil(\n",
    "#       num_timesteps\n",
    "#       / (\n",
    "#           num_evals_after_init\n",
    "#           * env_step_per_training_step\n",
    "#           * max(num_resets_per_eval, 1)\n",
    "#       )\n",
    "#   ).astype(int)\n",
    "\n",
    "#   key = jax.random.PRNGKey(seed)\n",
    "#   global_key, local_key = jax.random.split(key)\n",
    "#   del key\n",
    "#   local_key = jax.random.fold_in(local_key, process_id)\n",
    "#   local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
    "#   # key_networks should be global, so that networks are initialized the same\n",
    "#   # way for different processes.\n",
    "#   key_policy, key_value, key_mem = jax.random.split(global_key, 3)\n",
    "#   del global_key\n",
    "\n",
    "#   assert num_envs % device_count == 0\n",
    "\n",
    "#   v_randomization_fn = None\n",
    "#   if randomization_fn is not None:\n",
    "#     randomization_batch_size = num_envs // local_device_count\n",
    "#     # all devices gets the same randomization rng\n",
    "#     randomization_rng = jax.random.split(key_env, randomization_batch_size)\n",
    "#     v_randomization_fn = functools.partial(\n",
    "#         randomization_fn, rng=randomization_rng\n",
    "#     )\n",
    "\n",
    "#   if isinstance(environment, envs.Env):\n",
    "#     wrap_for_training = envs.training.wrap\n",
    "#   else:\n",
    "#     wrap_for_training = envs_v1.wrappers.wrap_for_training\n",
    "\n",
    "#   env = wrap_for_training(\n",
    "#       environment,\n",
    "#       episode_length=episode_length,\n",
    "#       action_repeat=action_repeat,\n",
    "#       randomization_fn=v_randomization_fn,\n",
    "#   )\n",
    "\n",
    "#   reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "#   key_envs = jax.random.split(key_env, num_envs // process_count)\n",
    "#   key_envs = jnp.reshape(key_envs,\n",
    "#                          (local_devices_to_use, -1) + key_envs.shape[1:])\n",
    "#   env_state = reset_fn(key_envs)\n",
    "\n",
    "#   normalize = lambda x, y: x\n",
    "#   if normalize_observations:\n",
    "#     normalize = running_statistics.normalize\n",
    "    \n",
    "#   ppo_network = network_factory(\n",
    "#       env_state.obs.shape[-1],\n",
    "#       env.action_size,\n",
    "#       preprocess_observations_fn=normalize)\n",
    "#   make_policy = make_lstm__inference_fn(ppo_network)\n",
    "\n",
    "#   optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "#   loss_fn = functools.partial(\n",
    "#       compute_lstm_ppo_loss,\n",
    "#       ppo_network=ppo_network,\n",
    "#       entropy_cost=entropy_cost,\n",
    "#       discounting=discounting,\n",
    "#       reward_scaling=reward_scaling,\n",
    "#       gae_lambda=gae_lambda,\n",
    "#       clipping_epsilon=clipping_epsilon,\n",
    "#       normalize_advantage=normalize_advantage)\n",
    "\n",
    "#   gradient_update_fn = gradients.gradient_update_fn(\n",
    "#       loss_fn, optimizer, pmap_axis_name=_PMAP_AXIS_NAME, has_aux=True)\n",
    "\n",
    "#   def minibatch_step(\n",
    "#       carry, data: types.Transition,\n",
    "#       normalizer_params: running_statistics.RunningStatisticsState):\n",
    "#     optimizer_state, params, key = carry\n",
    "#     key, key_loss = jax.random.split(key)\n",
    "#     (_, metrics), params, optimizer_state = gradient_update_fn(\n",
    "#         params,\n",
    "#         normalizer_params,\n",
    "#         data,\n",
    "#         key_loss,\n",
    "#         optimizer_state=optimizer_state)\n",
    "\n",
    "#     return (optimizer_state, params, key), metrics\n",
    "\n",
    "#   def sgd_step(carry, unused_t, data: types.Transition,\n",
    "#                normalizer_params: running_statistics.RunningStatisticsState):\n",
    "#     optimizer_state, params, key = carry\n",
    "#     key, key_perm, key_grad = jax.random.split(key, 3)\n",
    "\n",
    "#     def convert_data(x: jnp.ndarray):\n",
    "#       x = jax.random.permutation(key_perm, x)\n",
    "#       x = jnp.reshape(x, (num_minibatches, -1) + x.shape[1:])\n",
    "#       return x\n",
    "\n",
    "#     shuffled_data = jax.tree_util.tree_map(convert_data, data)\n",
    "#     (optimizer_state, params, _), metrics = jax.lax.scan(\n",
    "#         functools.partial(minibatch_step, normalizer_params=normalizer_params),\n",
    "#         (optimizer_state, params, key_grad),\n",
    "#         shuffled_data,\n",
    "#         length=num_minibatches)\n",
    "#     return (optimizer_state, params, key), metrics\n",
    "\n",
    "#   def training_step(\n",
    "#       carry: Tuple[TrainingState, envs.State, PRNGKey],\n",
    "#       unused_t) -> Tuple[Tuple[TrainingState, envs.State, PRNGKey], Metrics]:\n",
    "#     training_state, state, key = carry\n",
    "#     key_sgd, key_generate_unroll, new_key = jax.random.split(key, 3)\n",
    "\n",
    "#     policy = make_policy(\n",
    "#         (training_state.normalizer_params, training_state.params.mem, training_state.params.policy))\n",
    "\n",
    "#     def f(carry, unused_t):\n",
    "#       current_state, current_key = carry\n",
    "#       current_key, next_key = jax.random.split(current_key)\n",
    "#       next_state, data = acting.generate_unroll(\n",
    "#           env,\n",
    "#           current_state,\n",
    "#           policy,\n",
    "#           current_key,\n",
    "#           unroll_length,\n",
    "#           extra_fields=('truncation',))\n",
    "#       return (next_state, next_key), data\n",
    "\n",
    "#     (state, _), data = jax.lax.scan(\n",
    "#         f, (state, key_generate_unroll), (),\n",
    "#         length=batch_size * num_minibatches // num_envs)\n",
    "#     # Have leading dimensions (batch_size * num_minibatches, unroll_length)\n",
    "#     data = jax.tree_util.tree_map(lambda x: jnp.swapaxes(x, 1, 2), data)\n",
    "#     data = jax.tree_util.tree_map(lambda x: jnp.reshape(x, (-1,) + x.shape[2:]),\n",
    "#                                   data)\n",
    "\n",
    "#     assert data.discount.shape[1:] == (unroll_length,)\n",
    "\n",
    "#     # Update normalization params and normalize observations.\n",
    "#     normalizer_params = running_statistics.update(\n",
    "#         training_state.normalizer_params,\n",
    "#         data.observation,\n",
    "#         pmap_axis_name=_PMAP_AXIS_NAME)\n",
    "\n",
    "#     (optimizer_state, params, _), metrics = jax.lax.scan(\n",
    "#         functools.partial(\n",
    "#             sgd_step, data=data, normalizer_params=normalizer_params),\n",
    "#         (training_state.optimizer_state, training_state.params, key_sgd), (),\n",
    "#         length=num_updates_per_batch)\n",
    "\n",
    "#     new_training_state = TrainingState(\n",
    "#         optimizer_state=optimizer_state,\n",
    "#         params=params,\n",
    "#         normalizer_params=normalizer_params,\n",
    "#         env_steps=training_state.env_steps + env_step_per_training_step)\n",
    "#     return (new_training_state, state, new_key), metrics\n",
    "\n",
    "#   def training_epoch(training_state: TrainingState, state: envs.State,\n",
    "#                      key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
    "#     (training_state, state, _), loss_metrics = jax.lax.scan(\n",
    "#         training_step, (training_state, state, key), (),\n",
    "#         length=num_training_steps_per_epoch)\n",
    "#     loss_metrics = jax.tree_util.tree_map(jnp.mean, loss_metrics)\n",
    "#     return training_state, state, loss_metrics\n",
    "\n",
    "#   training_epoch = jax.pmap(training_epoch, axis_name=_PMAP_AXIS_NAME)\n",
    "\n",
    "#   # Note that this is NOT a pure jittable method.\n",
    "#   def training_epoch_with_timing(\n",
    "#       training_state: TrainingState, env_state: envs.State,\n",
    "#       key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
    "#     nonlocal training_walltime\n",
    "#     t = time.time()\n",
    "#     training_state, env_state = _strip_weak_type((training_state, env_state))\n",
    "#     result = training_epoch(training_state, env_state, key)\n",
    "#     training_state, env_state, metrics = _strip_weak_type(result)\n",
    "\n",
    "#     metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n",
    "#     jax.tree_util.tree_map(lambda x: x.block_until_ready(), metrics)\n",
    "\n",
    "#     epoch_training_time = time.time() - t\n",
    "#     training_walltime += epoch_training_time\n",
    "#     sps = (num_training_steps_per_epoch *\n",
    "#            env_step_per_training_step *\n",
    "#            max(num_resets_per_eval, 1)) / epoch_training_time\n",
    "#     metrics = {\n",
    "#         'training/sps': sps,\n",
    "#         'training/walltime': training_walltime,\n",
    "#         **{f'training/{name}': value for name, value in metrics.items()}\n",
    "#     }\n",
    "#     return training_state, env_state, metrics  # pytype: disable=bad-return-type  # py311-upgrade\n",
    "\n",
    "#   init_params = LSTM_PPONetworkParams(\n",
    "#       policy=ppo_network.policy_network.init(key_policy),\n",
    "#       value=ppo_network.value_network.init(key_value),\n",
    "#       mem=ppo_network.mem_network.init(key_mem))\n",
    "#   training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "#       optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
    "#       params=init_params,\n",
    "#       normalizer_params=running_statistics.init_state(\n",
    "#           specs.Array(env_state.obs.shape[-1:], jnp.dtype('float32'))),\n",
    "#       env_steps=0)\n",
    "#   training_state = jax.device_put_replicated(\n",
    "#       training_state,\n",
    "#       jax.local_devices()[:local_devices_to_use])\n",
    "\n",
    "#   if not eval_env:\n",
    "#     eval_env = environment\n",
    "#   if randomization_fn is not None:\n",
    "#     v_randomization_fn = functools.partial(\n",
    "#         randomization_fn, rng=jax.random.split(eval_key, num_eval_envs)\n",
    "#     )\n",
    "#   eval_env = wrap_for_training(\n",
    "#       eval_env,\n",
    "#       episode_length=episode_length,\n",
    "#       action_repeat=action_repeat,\n",
    "#       randomization_fn=v_randomization_fn,\n",
    "#   )\n",
    "\n",
    "#   evaluator = acting.Evaluator(\n",
    "#       eval_env,\n",
    "#       functools.partial(make_policy, deterministic=deterministic_eval),\n",
    "#       num_eval_envs=num_eval_envs,\n",
    "#       episode_length=episode_length,\n",
    "#       action_repeat=action_repeat,\n",
    "#       key=eval_key)\n",
    "\n",
    "#   # Run initial eval\n",
    "#   metrics = {}\n",
    "#   if process_id == 0 and num_evals > 1:\n",
    "#     metrics = evaluator.run_evaluation(\n",
    "#         _unpmap(\n",
    "#             (training_state.normalizer_params, training_state.params.mem, training_state.params.policy)),\n",
    "#         training_metrics={})\n",
    "#     logging.info(metrics)\n",
    "#     progress_fn(0, metrics)\n",
    "\n",
    "#   training_metrics = {}\n",
    "#   training_walltime = 0\n",
    "#   current_step = 0\n",
    "#   for it in range(num_evals_after_init):\n",
    "#     logging.info('starting iteration %s %s', it, time.time() - xt)\n",
    "\n",
    "#     for _ in range(max(num_resets_per_eval, 1)):\n",
    "#       # optimization\n",
    "#       epoch_key, local_key = jax.random.split(local_key)\n",
    "#       epoch_keys = jax.random.split(epoch_key, local_devices_to_use)\n",
    "#       (training_state, env_state, training_metrics) = (\n",
    "#           training_epoch_with_timing(training_state, env_state, epoch_keys)\n",
    "#       )\n",
    "#       current_step = int(_unpmap(training_state.env_steps))\n",
    "\n",
    "#       key_envs = jax.vmap(\n",
    "#           lambda x, s: jax.random.split(x[0], s),\n",
    "#           in_axes=(0, None))(key_envs, key_envs.shape[1])\n",
    "#       # TODO: move extra reset logic to the AutoResetWrapper.\n",
    "#       env_state = reset_fn(key_envs) if num_resets_per_eval > 0 else env_state\n",
    "\n",
    "#     if process_id == 0:\n",
    "#       # Run evals.\n",
    "#       metrics = evaluator.run_evaluation(\n",
    "#           _unpmap(\n",
    "#               (training_state.normalizer_params, training_state.params.mem, training_state.params.policy)),\n",
    "#           training_metrics)\n",
    "#       logging.info(metrics)\n",
    "#       progress_fn(current_step, metrics)\n",
    "#       params = _unpmap((training_state.normalizer_params, training_state.params.mem, training_state.params.policy))\n",
    "#       policy_params_fn(current_step, make_policy, params)\n",
    "\n",
    "#   total_steps = current_step\n",
    "#   assert total_steps >= num_timesteps\n",
    "\n",
    "#   # If there was no mistakes the training_state should still be identical on all\n",
    "#   # devices.\n",
    "#   pmap.assert_is_replicated(training_state)\n",
    "#   params = _unpmap(\n",
    "#       (training_state.normalizer_params, training_state.params.mem, training_state.params.policy))\n",
    "#   logging.info('total steps: %s', total_steps)\n",
    "#   pmap.synchronize_hosts()\n",
    "#   return (make_policy, params, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Networks Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclasses.dataclass\n",
    "class FeedForwardNetwork:\n",
    "  init: Callable[..., Any]\n",
    "  apply: Callable[..., Any]\n",
    "\n",
    "ActivationFn = Callable[[jp.ndarray], jp.ndarray]\n",
    "Initializer = Callable[..., Any]\n",
    "\n",
    "def small_random_normal(stddev=0.00001):\n",
    "    \"\"\"Custom initializer for biases with small random values centered around zero.\"\"\"\n",
    "    def init(key, shape, dtype=jp.float32):\n",
    "        return jax.random.normal(key, shape, dtype=dtype) * stddev\n",
    "    return init\n",
    "\n",
    "def identity_like_init(scale=0.00001, c=5):\n",
    "    \"\"\"Custom initializer for near-identity matrices.\"\"\"\n",
    "    def init(key, shape, dtype=jp.float32,i=1, num=1):\n",
    "        assert shape[0] == shape[1], \"Identity-like init is designed for square matrices only\"\n",
    "        # Create a matrix that is mostly zeros\n",
    "        matrix = jp.identity(shape[0], dtype=dtype) * c\n",
    "        if i == 0:\n",
    "          matrix = matrix * 1/(c**num)\n",
    "        noise = jax.random.uniform(key, shape, dtype=dtype, minval=-scale, maxval=scale)\n",
    "        return matrix + noise\n",
    "    return init\n",
    "\n",
    "class IMLP(linen.Module):\n",
    "    layer_sizes: Sequence[int]\n",
    "    activation: ActivationFn = linen.tanh\n",
    "    kernel_init: Initializer = identity_like_init(scale=0.00001)\n",
    "    bias_init: Initializer = small_random_normal(stddev=0.00001)\n",
    "    bias: bool = True\n",
    "    \n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, data: jp.ndarray):\n",
    "        hidden = data\n",
    "        for i, hidden_size in enumerate(self.layer_sizes):\n",
    "            assert hidden_size == data.shape[-1], \"For identity init, all layers must match input size\"\n",
    "            hidden = linen.Dense(\n",
    "                hidden_size,\n",
    "                name=f'hidden_{i}',\n",
    "                kernel_init=functools.partial(self.kernel_init,i=i, num=len(self.layer_sizes)),\n",
    "                bias_init=self.bias_init,\n",
    "                use_bias=self.bias)(\n",
    "                    hidden)\n",
    "            if i != len(self.layer_sizes) - 1:\n",
    "                hidden = self.activation(hidden)\n",
    "        return hidden\n",
    "\n",
    "\n",
    "# class MLP(linen.Module):\n",
    "#   \"\"\"MLP module.\"\"\"\n",
    "#   layer_sizes: Sequence[int]\n",
    "#   activation: ActivationFn = linen.relu\n",
    "#   kernel_init: Initializer = jax.nn.initializers.lecun_uniform()\n",
    "#   activate_final: bool = False\n",
    "#   bias: bool = True\n",
    "\n",
    "#   @linen.compact\n",
    "#   def __call__(self, data: jp.ndarray):\n",
    "#     hidden = data\n",
    "#     for i, hidden_size in enumerate(self.layer_sizes):\n",
    "#       hidden = linen.Dense(\n",
    "#           hidden_size,\n",
    "#           name=f'hidden_{i}',\n",
    "#           kernel_init=self.kernel_init,\n",
    "#           use_bias=self.bias)(\n",
    "#               hidden)\n",
    "#       if i != len(self.layer_sizes) - 1 or self.activate_final:\n",
    "#         hidden = self.activation(hidden)\n",
    "#     return hidden\n",
    "\n",
    "def make_random_network(\n",
    "    obs_size: int,\n",
    "    hidden_layer_sizes: Sequence[int] = (256, 256),\n",
    "    activation: ActivationFn = linen.tanh,\n",
    "    scale=0.00001) -> FeedForwardNetwork:\n",
    "  \"\"\"Creates a policy network.\"\"\"\n",
    "\n",
    "  policy_module = IMLP(layer_sizes=list(hidden_layer_sizes),activation=activation,kernel_init=identity_like_init(scale=scale),bias_init=small_random_normal(stddev=scale))\n",
    "\n",
    "  def apply(rand_params, obs):\n",
    "    return policy_module.apply(rand_params, obs)\n",
    "\n",
    "  dummy_obs = jp.zeros((1, obs_size))\n",
    "  return FeedForwardNetwork(\n",
    "      init=lambda key: policy_module.init(key, dummy_obs), apply=apply)\n",
    "\n",
    "class Encoder(linen.Module):\n",
    "    layer_sizes: Sequence[int]  # Sizes of each hidden layer\n",
    "    activation: Callable[[jp.ndarray], jp.ndarray] = linen.relu  # Default activation function\n",
    "    activate_final: bool = False\n",
    "\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, data):\n",
    "        for i,size in enumerate(self.layer_sizes):\n",
    "            z = linen.Dense(size)(data)\n",
    "            if i != len(self.layer_sizes) - 1 or self.activate_final:\n",
    "              z = self.activation(z)\n",
    "        return z\n",
    "\n",
    "class Decoder(linen.Module):\n",
    "    layer_sizes: Sequence[int]  # Sizes of each hidden layer, typically reverse of the encoder\n",
    "    activation: Callable[[jp.ndarray], jp.ndarray] = linen.relu  # Default activation function\n",
    "\n",
    "    @linen.compact\n",
    "    def __call__(self, data):\n",
    "        for i,size in enumerate(self.layer_sizes):\n",
    "            z = linen.Dense(size)(data)\n",
    "            if i != len(self.layer_sizes) - 1 or self.activate_final:\n",
    "              z = self.activation(z)\n",
    "            z = self.activation(z)\n",
    "        return z\n",
    "    \n",
    "def make_encode_decode(\n",
    "    data_size_encode: int,\n",
    "    data_size_decode: int,\n",
    "    squash_size: int,\n",
    "    hidden_layer_sizes_encode: Sequence[int] = (256, 256),\n",
    "    hidden_layer_sizes_decode: Sequence[int] = (256, 256),\n",
    "    activation: ActivationFn = linen.tanh,\n",
    "    ) -> Union[FeedForwardNetwork,FeedForwardNetwork]:\n",
    "\n",
    "  policy_module_encode = Encoder(layer_sizes=list(jp.concat((jp.array(hidden_layer_sizes_encode),jp.array([squash_size])))),activation=activation)\n",
    "  policy_module_decode = Encoder(layer_sizes=list(jp.concat((jp.array(hidden_layer_sizes_decode),jp.array([data_size_decode])))),activation=activation)\n",
    "\n",
    "\n",
    "  def apply_encode(params, data):\n",
    "    return policy_module_encode.apply(params, data)\n",
    "  \n",
    "  def apply_decode(params, data):\n",
    "    return policy_module_decode.apply(params, data)\n",
    "\n",
    "  dummy_en = jp.zeros((1, data_size_encode))\n",
    "  dummy_de = jp.zeros((1, squash_size))\n",
    "\n",
    "  encode = FeedForwardNetwork(\n",
    "      init=lambda key: policy_module_encode.init(key, dummy_en), apply=apply_encode)\n",
    "  \n",
    "  decode = FeedForwardNetwork(\n",
    "      init=lambda key: policy_module_decode.init(key, dummy_de), apply=apply_decode)\n",
    "  \n",
    "  return (encode,decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_auto_encoder(p1,p2):\n",
    "    total_con = []\n",
    "\n",
    "    for layer in p1:\n",
    "        hidden1 = p1[layer]\n",
    "\n",
    "        k1 = hidden1['kernel']\n",
    "        b1 = hidden1['bias']\n",
    "\n",
    "        if len(k1.shape) == 4:\n",
    "\n",
    "            flattened1 = k1.reshape(k1.shape[0], k1.shape[1], k1.shape[2]*k1.shape[3])\n",
    "\n",
    "            con1 = jp.concatenate([flattened1, b1], axis=2)\n",
    "\n",
    "            if len(total_con) == 0:\n",
    "                total_con = con1\n",
    "            else:\n",
    "                total_con = jp.concatenate([total_con,con1],axis=2)\n",
    "        \n",
    "        else:\n",
    "            flattened1 = k1.reshape(k1.shape[0]*k1.shape[0])\n",
    "\n",
    "            con1 = jp.concatenate([flattened1, b1])\n",
    "\n",
    "            if len(total_con) == 0:\n",
    "                total_con = con1\n",
    "            else:\n",
    "                total_con = jp.concatenate([total_con,con1])\n",
    "\n",
    "    for layer in p2:\n",
    "        hidden2 = p2[layer]\n",
    "\n",
    "        k2 = hidden2['kernel']\n",
    "        b2 = hidden2['bias']\n",
    "\n",
    "        if len(k2.shape) == 4:\n",
    "\n",
    "            flattened2 = k2.reshape(k2.shape[0], k2.shape[1], k2.shape[2]*k2.shape[3])\n",
    "\n",
    "            con2 = jp.concatenate([flattened2, b2], axis=2)\n",
    "\n",
    "            total_con = jp.concatenate([total_con,con2],axis=2)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            flattened2 = k2.reshape(k2.shape[0]*k2.shape[0])\n",
    "\n",
    "            con2 = jp.concatenate([flattened2, b2])\n",
    "\n",
    "            total_con = jp.concatenate([total_con,con2])\n",
    "\n",
    "    return total_con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_flatten_single(flat,p1_size,p2_size,p1_len,p2_len):\n",
    "    \n",
    "    offset = 0\n",
    "\n",
    "    p1 = {}\n",
    "    p2 = {}\n",
    "\n",
    "    for i in range(p1_len):\n",
    "        p1[f'hidden_{i}'] = {}\n",
    "        p1[f'hidden_{i}']['kernel'] = flat[offset:p1_size*p1_size+offset].reshape(p1_size,p1_size)\n",
    "        offset += p1_size*p1_size\n",
    "        p1[f'hidden_{i}']['bias'] = flat[offset:p1_size+offset]\n",
    "        offset += p1_size\n",
    "\n",
    "    for i in range(p2_len):\n",
    "        p2[f'hidden_{i}'] = {}\n",
    "        p2[f'hidden_{i}']['kernel'] = flat[offset:p2_size*p2_size+offset].reshape(p2_size,p2_size)\n",
    "        offset += p2_size*p2_size\n",
    "        p2[f'hidden_{i}']['bias'] = flat[offset:p2_size+offset]\n",
    "        offset += p2_size\n",
    "\n",
    "    return p1,p2\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_encoder_loss(params,data, en_obs, en_net, de_obs, de_net):\n",
    "    obs_encoder =  params.obs_encoder\n",
    "    obs_decoder =  params.obs_decoder\n",
    "    net_encoder = params.net_encoder\n",
    "    net_decoder = params.net_decoder\n",
    "\n",
    "    p1 = data[5]['state_extras']['p1']['params']\n",
    "    p2 = data[5]['state_extras']['p2']['params']\n",
    "\n",
    "    obs = data[0]\n",
    "    net = flatten_auto_encoder(p1,p2)\n",
    "\n",
    "    # Encode the inputs using both encoders\n",
    "    encoded_obs = en_obs.apply(obs_encoder, obs)\n",
    "    encoded_net = en_net.apply(net_encoder, net)\n",
    "    \n",
    "    # Decode the encoded representations\n",
    "    reconstructed_obs = de_obs.apply(obs_decoder, encoded_obs)\n",
    "    reconstructed_net = de_net.apply(net_decoder, encoded_net)\n",
    "    \n",
    "    # Reconstruction loss (mean squared error)\n",
    "    recon_loss1 = jp.mean((reconstructed_obs - obs) ** 2)\n",
    "    recon_loss2 = jp.mean((reconstructed_net - net) ** 2)\n",
    "    \n",
    "    # Similarity loss (mean squared error between embeddings)\n",
    "    similarity_loss = jp.mean((encoded_obs - encoded_net) ** 2)\n",
    "    \n",
    "    # Total loss is a weighted sum of reconstruction loss and similarity loss\n",
    "    total_loss = recon_loss1 + recon_loss2 + similarity_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InferenceParams = Tuple[running_statistics.NestedMeanStd, Params]\n",
    "Metrics = types.Metrics\n",
    "\n",
    "_PMAP_AXIS_NAME = 'i'\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class AutoEncoderParams:\n",
    "  \"\"\"Contains training state for the learner.\"\"\"\n",
    "  obs_encoder: Params\n",
    "  net_encoder: Params\n",
    "  obs_decoder: Params\n",
    "  net_decoder: Params\n",
    "\n",
    "@flax.struct.dataclass\n",
    "class TrainingState:\n",
    "  \"\"\"Contains training state for the learner.\"\"\"\n",
    "  optimizer_state: optax.OptState\n",
    "  auto_optimizer_state: optax.OptState\n",
    "  params: ppo_losses.PPONetworkParams\n",
    "  auto_params: AutoEncoderParams\n",
    "  normalizer_params: running_statistics.RunningStatisticsState\n",
    "  env_steps: jp.ndarray\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _unpmap(v):\n",
    "  return jax.tree_util.tree_map(lambda x: x[0], v)\n",
    "\n",
    "\n",
    "def _strip_weak_type(tree):\n",
    "  # brax user code is sometimes ambiguous about weak_type.  in order to\n",
    "  # avoid extra jit recompilations we strip all weak types from user input\n",
    "  def f(leaf):\n",
    "    leaf = jp.asarray(leaf)\n",
    "    return leaf.astype(leaf.dtype)\n",
    "  return jax.tree_util.tree_map(f, tree)\n",
    "\n",
    "\n",
    "def train(\n",
    "    environment: Union[envs_v1.Env, envs.Env],\n",
    "    num_timesteps: int,\n",
    "    episode_length: int,\n",
    "    action_repeat: int = 1,\n",
    "    num_envs: int = 1,\n",
    "    max_devices_per_host: Optional[int] = None,\n",
    "    num_eval_envs: int = 128,\n",
    "    learning_rate: float = 1e-4,\n",
    "    entropy_cost: float = 1e-4,\n",
    "    discounting: float = 0.9,\n",
    "    seed: int = 0,\n",
    "    unroll_length: int = 10,\n",
    "    batch_size: int = 32,\n",
    "    num_minibatches: int = 16,\n",
    "    num_updates_per_batch: int = 2,\n",
    "    num_evals: int = 1,\n",
    "    num_resets_per_eval: int = 0,\n",
    "    normalize_observations: bool = False,\n",
    "    reward_scaling: float = 1.0,\n",
    "    clipping_epsilon: float = 0.3,\n",
    "    gae_lambda: float = 0.95,\n",
    "    deterministic_eval: bool = False,\n",
    "    network_factory: types.NetworkFactory[\n",
    "        ppo_networks.PPONetworks\n",
    "    ] = ppo_networks.make_ppo_networks,\n",
    "    auto_encoder_learning_rate = 1e-4,\n",
    "    auto_encode_factory = make_encode_decode,\n",
    "    progress_fn: Callable[[int, Metrics], None] = lambda *args: None,\n",
    "    normalize_advantage: bool = True,\n",
    "    eval_env: Optional[envs.Env] = None,\n",
    "    policy_params_fn: Callable[..., None] = lambda *args: None,\n",
    "    randomization_fn: Optional[\n",
    "        Callable[[base.System, jp.ndarray], Tuple[base.System, base.System]]\n",
    "    ] = None,\n",
    "):\n",
    "  \"\"\"PPO training.\n",
    "\n",
    "  Args:\n",
    "    environment: the environment to train\n",
    "    num_timesteps: the total number of environment steps to use during training\n",
    "    episode_length: the length of an environment episode\n",
    "    action_repeat: the number of timesteps to repeat an action\n",
    "    num_envs: the number of parallel environments to use for rollouts\n",
    "      NOTE: `num_envs` must be divisible by the total number of chips since each\n",
    "        chip gets `num_envs // total_number_of_chips` environments to roll out\n",
    "      NOTE: `batch_size * num_minibatches` must be divisible by `num_envs` since\n",
    "        data generated by `num_envs` parallel envs gets used for gradient\n",
    "        updates over `num_minibatches` of data, where each minibatch has a\n",
    "        leading dimension of `batch_size`\n",
    "    max_devices_per_host: maximum number of chips to use per host process\n",
    "    num_eval_envs: the number of envs to use for evluation. Each env will run 1\n",
    "      episode, and all envs run in parallel during eval.\n",
    "    learning_rate: learning rate for ppo loss\n",
    "    entropy_cost: entropy reward for ppo loss, higher values increase entropy\n",
    "      of the policy\n",
    "    discounting: discounting rate\n",
    "    seed: random seed\n",
    "    unroll_length: the number of timesteps to unroll in each environment. The\n",
    "      PPO loss is computed over `unroll_length` timesteps\n",
    "    batch_size: the batch size for each minibatch SGD step\n",
    "    num_minibatches: the number of times to run the SGD step, each with a\n",
    "      different minibatch with leading dimension of `batch_size`\n",
    "    num_updates_per_batch: the number of times to run the gradient update over\n",
    "      all minibatches before doing a new environment rollout\n",
    "    num_evals: the number of evals to run during the entire training run.\n",
    "      Increasing the number of evals increases total training time\n",
    "    num_resets_per_eval: the number of environment resets to run between each\n",
    "      eval. The environment resets occur on the host\n",
    "    normalize_observations: whether to normalize observations\n",
    "    reward_scaling: float scaling for reward\n",
    "    clipping_epsilon: clipping epsilon for PPO loss\n",
    "    gae_lambda: General advantage estimation lambda\n",
    "    deterministic_eval: whether to run the eval with a deterministic policy\n",
    "    network_factory: function that generates networks for policy and value\n",
    "      functions\n",
    "    progress_fn: a user-defined callback function for reporting/plotting metrics\n",
    "    normalize_advantage: whether to normalize advantage estimate\n",
    "    eval_env: an optional environment for eval only, defaults to `environment`\n",
    "    policy_params_fn: a user-defined callback function that can be used for\n",
    "      saving policy checkpoints\n",
    "    randomization_fn: a user-defined callback function that generates randomized\n",
    "      environments\n",
    "\n",
    "  Returns:\n",
    "    Tuple of (make_policy function, network params, metrics)\n",
    "  \"\"\"\n",
    "  assert batch_size * num_minibatches % num_envs == 0\n",
    "  xt = time.time()\n",
    "\n",
    "  process_count = jax.process_count()\n",
    "  process_id = jax.process_index()\n",
    "  local_device_count = jax.local_device_count()\n",
    "  local_devices_to_use = local_device_count\n",
    "  if max_devices_per_host:\n",
    "    local_devices_to_use = min(local_devices_to_use, max_devices_per_host)\n",
    "  logging.info(\n",
    "      'Device count: %d, process count: %d (id %d), local device count: %d, '\n",
    "      'devices to be used count: %d', jax.device_count(), process_count,\n",
    "      process_id, local_device_count, local_devices_to_use)\n",
    "  device_count = local_devices_to_use * process_count\n",
    "\n",
    "  # The number of environment steps executed for every training step.\n",
    "  env_step_per_training_step = (\n",
    "      batch_size * unroll_length * num_minibatches * action_repeat)\n",
    "  num_evals_after_init = max(num_evals - 1, 1)\n",
    "  # The number of training_step calls per training_epoch call.\n",
    "  # equals to ceil(num_timesteps / (num_evals * env_step_per_training_step *\n",
    "  #                                 num_resets_per_eval))\n",
    "  num_training_steps_per_epoch = np.ceil(\n",
    "      num_timesteps\n",
    "      / (\n",
    "          num_evals_after_init\n",
    "          * env_step_per_training_step\n",
    "          * max(num_resets_per_eval, 1)\n",
    "      )\n",
    "  ).astype(int)\n",
    "\n",
    "  key = jax.random.PRNGKey(seed)\n",
    "  global_key, local_key = jax.random.split(key)\n",
    "  del key\n",
    "  local_key = jax.random.fold_in(local_key, process_id)\n",
    "  local_key, key_env, eval_key = jax.random.split(local_key, 3)\n",
    "  # key_networks should be global, so that networks are initialized the same\n",
    "  # way for different processes.\n",
    "  key_policy, key_value, key1, key2, key3, key4 = jax.random.split(global_key, 6)\n",
    "  del global_key\n",
    "\n",
    "  assert num_envs % device_count == 0\n",
    "\n",
    "  v_randomization_fn = None\n",
    "  if randomization_fn is not None:\n",
    "    randomization_batch_size = num_envs // local_device_count\n",
    "    # all devices gets the same randomization rng\n",
    "    randomization_rng = jax.random.split(key_env, randomization_batch_size)\n",
    "    v_randomization_fn = functools.partial(\n",
    "        randomization_fn, rng=randomization_rng\n",
    "    )\n",
    "\n",
    "  if isinstance(environment, envs.Env):\n",
    "    wrap_for_training = envs.training.wrap\n",
    "  else:\n",
    "    wrap_for_training = envs_v1.wrappers.wrap_for_training\n",
    "\n",
    "  env = wrap_for_training(\n",
    "      environment,\n",
    "      episode_length=episode_length,\n",
    "      action_repeat=action_repeat,\n",
    "      randomization_fn=v_randomization_fn,\n",
    "  )\n",
    "\n",
    "  reset_fn = jax.jit(jax.vmap(env.reset))\n",
    "  key_envs = jax.random.split(key_env, num_envs // process_count)\n",
    "  key_envs = jp.reshape(key_envs,\n",
    "                         (local_devices_to_use, -1) + key_envs.shape[1:])\n",
    "  env_state = reset_fn(key_envs)\n",
    "\n",
    "  normalize = lambda x, y: x\n",
    "  if normalize_observations:\n",
    "    normalize = running_statistics.normalize\n",
    "  ppo_network = network_factory(\n",
    "      env_state.obs.shape[-1],\n",
    "      env.action_size,\n",
    "      preprocess_observations_fn=normalize)\n",
    "  make_policy = ppo_networks.make_inference_fn(ppo_network)\n",
    "\n",
    "  if env_state.info['p1'] != 0:\n",
    "    pcount = flatten_auto_encoder(env_state.info['p1']['params'],env_state.info['p2']['params']).shape[-1]\n",
    "    obs_encoder, obs_decoder = auto_encode_factory(\n",
    "      data_size_encode = env_state.obs.shape[-1],\n",
    "      data_size_decode = env_state.obs.shape[-1])\n",
    "\n",
    "    net_encoder, net_decoder = auto_encode_factory(\n",
    "    data_size_encode = pcount,\n",
    "    data_size_decode =pcount)\n",
    "  else:\n",
    "    pcount = 1\n",
    "    obs_encoder, obs_decoder = auto_encode_factory(\n",
    "      data_size_encode = 1,\n",
    "      data_size_decode = 1)\n",
    "\n",
    "    net_encoder, net_decoder = auto_encode_factory(\n",
    "    data_size_encode = pcount,\n",
    "    data_size_decode =pcount)\n",
    "  \n",
    "\n",
    "  auto_optimizer = optax.adam(learning_rate=auto_encoder_learning_rate)\n",
    "\n",
    "  auto_encode_loss_fn = functools.partial(\n",
    "    auto_encoder_loss,\n",
    "    en_obs=obs_encoder,\n",
    "    en_net=net_encoder,\n",
    "    de_obs=obs_decoder,\n",
    "    de_net=net_decoder)\n",
    "  \n",
    "  auto_gradient_update_fn = gradients.gradient_update_fn(auto_encode_loss_fn, auto_optimizer, pmap_axis_name=_PMAP_AXIS_NAME, has_aux=False)\n",
    "\n",
    "  optimizer = optax.adam(learning_rate=learning_rate)\n",
    "\n",
    "  loss_fn = functools.partial(\n",
    "      ppo_losses.compute_ppo_loss,\n",
    "      ppo_network=ppo_network,\n",
    "      entropy_cost=entropy_cost,\n",
    "      discounting=discounting,\n",
    "      reward_scaling=reward_scaling,\n",
    "      gae_lambda=gae_lambda,\n",
    "      clipping_epsilon=clipping_epsilon,\n",
    "      normalize_advantage=normalize_advantage)\n",
    "\n",
    "  gradient_update_fn = gradients.gradient_update_fn(loss_fn, optimizer, pmap_axis_name=_PMAP_AXIS_NAME, has_aux=True)\n",
    "\n",
    "  def minibatch_step(carry, data: types.Transition, normalizer_params: running_statistics.RunningStatisticsState):\n",
    "    optimizer_state, params, auto_opt, auto_params, key = carry\n",
    "    key, key_loss= jax.random.split(key)\n",
    "    (_, metrics), params, optimizer_state = gradient_update_fn(\n",
    "        params,\n",
    "        normalizer_params,\n",
    "        data,\n",
    "        key_loss,\n",
    "        optimizer_state=optimizer_state)\n",
    "    \n",
    "    if pcount > 1:\n",
    "      auto_loss, auto_params, auto_opt = auto_gradient_update_fn(\n",
    "          auto_params,\n",
    "          data,\n",
    "          optimizer_state=auto_opt)\n",
    "\n",
    "    return (optimizer_state, params, auto_opt, auto_params, key), metrics\n",
    "\n",
    "  def sgd_step(carry, unused_t, data: types.Transition,  normalizer_params: running_statistics.RunningStatisticsState):\n",
    "    optimizer_state, params, auto_opt, auto_params, key = carry\n",
    "    key, key_perm, key_grad = jax.random.split(key, 3)\n",
    "\n",
    "    def convert_data(x: jp.ndarray):\n",
    "      x = jax.random.permutation(key_perm, x)\n",
    "      x = jp.reshape(x, (num_minibatches, -1) + x.shape[1:])\n",
    "      return x\n",
    "\n",
    "    shuffled_data = jax.tree_util.tree_map(convert_data, data)\n",
    "    (optimizer_state, params, auto_opt, auto_params, _), metrics = jax.lax.scan(\n",
    "        functools.partial(minibatch_step, normalizer_params=normalizer_params),\n",
    "        (optimizer_state, params, auto_opt, auto_params, key_grad),\n",
    "        shuffled_data,\n",
    "        length=num_minibatches)\n",
    "    return (optimizer_state, params, auto_opt, auto_params, key), metrics\n",
    "\n",
    "  def training_step(\n",
    "      carry: Tuple[TrainingState, envs.State, PRNGKey],\n",
    "      unused_t) -> Tuple[Tuple[TrainingState, envs.State, PRNGKey], Metrics]:\n",
    "    training_state, state, key = carry\n",
    "    key_sgd, key_generate_unroll, new_key = jax.random.split(key, 3)\n",
    "\n",
    "    policy = make_policy(\n",
    "        (training_state.normalizer_params, training_state.params.policy))\n",
    "\n",
    "    def f(carry, unused_t):\n",
    "      current_state, current_key = carry\n",
    "      current_key, next_key = jax.random.split(current_key)\n",
    "      next_state, data = acting.generate_unroll(\n",
    "          env,\n",
    "          current_state,\n",
    "          policy,\n",
    "          current_key,\n",
    "          unroll_length,\n",
    "          extra_fields=('truncation','p1','p2',))\n",
    "      return (next_state, next_key), data\n",
    "\n",
    "    (state, _), data = jax.lax.scan(\n",
    "        f, (state, key_generate_unroll), (),\n",
    "        length=batch_size * num_minibatches // num_envs)\n",
    "    # Have leading dimensions (batch_size * num_minibatches, unroll_length)\n",
    "    data = jax.tree_util.tree_map(lambda x: jp.swapaxes(x, 1, 2), data)\n",
    "    data = jax.tree_util.tree_map(lambda x: jp.reshape(x, (-1,) + x.shape[2:]),\n",
    "                                  data)\n",
    "    assert data.discount.shape[1:] == (unroll_length,)\n",
    "\n",
    "    # Update normalization params and normalize observations.\n",
    "    normalizer_params = running_statistics.update(\n",
    "        training_state.normalizer_params,\n",
    "        data.observation,\n",
    "        pmap_axis_name=_PMAP_AXIS_NAME)\n",
    "\n",
    "    (optimizer_state, params, auto_optimizer_state, auto_params, _), metrics = jax.lax.scan(\n",
    "        functools.partial(\n",
    "            sgd_step, data=data, normalizer_params=normalizer_params),\n",
    "        (training_state.optimizer_state, training_state.params, training_state.auto_optimizer_state,training_state.auto_params, key_sgd), (),\n",
    "        length=num_updates_per_batch)\n",
    "\n",
    "    new_training_state = TrainingState(\n",
    "        optimizer_state=optimizer_state,\n",
    "        auto_optimizer_state=auto_optimizer_state, \n",
    "        auto_params = auto_params,\n",
    "        params=params,\n",
    "        normalizer_params=normalizer_params,\n",
    "        env_steps=training_state.env_steps + env_step_per_training_step)\n",
    "    return (new_training_state, state, new_key), metrics\n",
    "\n",
    "  def training_epoch(training_state: TrainingState, state: envs.State,\n",
    "                     key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
    "    (training_state, state, _), loss_metrics = jax.lax.scan(\n",
    "        training_step, (training_state, state, key), (),\n",
    "        length=num_training_steps_per_epoch)\n",
    "    loss_metrics = jax.tree_util.tree_map(jp.mean, loss_metrics)\n",
    "    return training_state, state, loss_metrics\n",
    "\n",
    "  training_epoch = jax.pmap(training_epoch, axis_name=_PMAP_AXIS_NAME)\n",
    "\n",
    "  # Note that this is NOT a pure jittable method.\n",
    "  def training_epoch_with_timing(\n",
    "      training_state: TrainingState, env_state: envs.State,\n",
    "      key: PRNGKey) -> Tuple[TrainingState, envs.State, Metrics]:\n",
    "    nonlocal training_walltime\n",
    "    t = time.time()\n",
    "    training_state, env_state = _strip_weak_type((training_state, env_state))\n",
    "    result = training_epoch(training_state, env_state, key)\n",
    "    training_state, env_state, metrics = _strip_weak_type(result)\n",
    "\n",
    "    metrics = jax.tree_util.tree_map(jp.mean, metrics)\n",
    "    jax.tree_util.tree_map(lambda x: x.block_until_ready(), metrics)\n",
    "\n",
    "    epoch_training_time = time.time() - t\n",
    "    training_walltime += epoch_training_time\n",
    "    sps = (num_training_steps_per_epoch *\n",
    "           env_step_per_training_step *\n",
    "           max(num_resets_per_eval, 1)) / epoch_training_time\n",
    "    metrics = {\n",
    "        'training/sps': sps,\n",
    "        'training/walltime': training_walltime,\n",
    "        **{f'training/{name}': value for name, value in metrics.items()}\n",
    "    }\n",
    "    return training_state, env_state, metrics  # pytype: disable=bad-return-type  # py311-upgrade\n",
    "\n",
    "  init_params = ppo_losses.PPONetworkParams(\n",
    "      policy=ppo_network.policy_network.init(key_policy),\n",
    "      value=ppo_network.value_network.init(key_value))\n",
    "  init_auto_params = AutoEncoderParams(\n",
    "      obs_encoder = obs_encoder.init(key1),\n",
    "      net_encoder = net_encoder.init(key2),\n",
    "      obs_decoder = obs_decoder.init(key3),\n",
    "      net_decoder = net_decoder.init(key4)\n",
    "  )\n",
    "  training_state = TrainingState(  # pytype: disable=wrong-arg-types  # jax-ndarray\n",
    "      optimizer_state=optimizer.init(init_params),  # pytype: disable=wrong-arg-types  # numpy-scalars\n",
    "      auto_optimizer_state=auto_optimizer.init(init_auto_params),\n",
    "      params=init_params,\n",
    "      auto_params = init_auto_params,\n",
    "      normalizer_params=running_statistics.init_state(\n",
    "          specs.Array(env_state.obs.shape[-1:], jp.dtype('float32'))),\n",
    "      env_steps=0)\n",
    "  training_state = jax.device_put_replicated(\n",
    "      training_state,\n",
    "      jax.local_devices()[:local_devices_to_use])\n",
    "\n",
    "  if not eval_env:\n",
    "    eval_env = environment\n",
    "  if randomization_fn is not None:\n",
    "    v_randomization_fn = functools.partial(\n",
    "        randomization_fn, rng=jax.random.split(eval_key, num_eval_envs)\n",
    "    )\n",
    "  eval_env = wrap_for_training(\n",
    "      eval_env,\n",
    "      episode_length=episode_length,\n",
    "      action_repeat=action_repeat,\n",
    "      randomization_fn=v_randomization_fn,\n",
    "  )\n",
    "\n",
    "  evaluator = acting.Evaluator(\n",
    "      eval_env,\n",
    "      functools.partial(make_policy, deterministic=deterministic_eval),\n",
    "      num_eval_envs=num_eval_envs,\n",
    "      episode_length=episode_length,\n",
    "      action_repeat=action_repeat,\n",
    "      key=eval_key)\n",
    "\n",
    "  # Run initial eval\n",
    "  metrics = {}\n",
    "  if process_id == 0 and num_evals > 1:\n",
    "    metrics = evaluator.run_evaluation(\n",
    "        _unpmap(\n",
    "            (training_state.normalizer_params, training_state.params.policy)),\n",
    "        training_metrics={})\n",
    "    logging.info(metrics)\n",
    "    progress_fn(0, metrics)\n",
    "\n",
    "  training_metrics = {}\n",
    "  training_walltime = 0\n",
    "  current_step = 0\n",
    "  for it in range(num_evals_after_init):\n",
    "    logging.info('starting iteration %s %s', it, time.time() - xt)\n",
    "\n",
    "    for _ in range(max(num_resets_per_eval, 1)):\n",
    "      # optimization\n",
    "      epoch_key, local_key = jax.random.split(local_key)\n",
    "      epoch_keys = jax.random.split(epoch_key, local_devices_to_use)\n",
    "      (training_state, env_state, training_metrics) = (\n",
    "          training_epoch_with_timing(training_state, env_state, epoch_keys)\n",
    "      )\n",
    "      current_step = int(_unpmap(training_state.env_steps))\n",
    "\n",
    "      key_envs = jax.vmap(\n",
    "          lambda x, s: jax.random.split(x[0], s),\n",
    "          in_axes=(0, None))(key_envs, key_envs.shape[1])\n",
    "      # TODO: move extra reset logic to the AutoResetWrapper.\n",
    "      env_state = reset_fn(key_envs) if num_resets_per_eval > 0 else env_state\n",
    "\n",
    "    if process_id == 0:\n",
    "      # Run evals.\n",
    "      metrics = evaluator.run_evaluation(\n",
    "          _unpmap(\n",
    "              (training_state.normalizer_params, training_state.params.policy)),\n",
    "          training_metrics)\n",
    "      logging.info(metrics)\n",
    "      progress_fn(current_step, metrics)\n",
    "      params = _unpmap(\n",
    "          (training_state.normalizer_params, training_state.params.policy))\n",
    "      policy_params_fn(current_step, make_policy, params)\n",
    "\n",
    "  total_steps = current_step\n",
    "  assert total_steps >= num_timesteps\n",
    "\n",
    "  # If there was no mistakes the training_state should still be identical on all\n",
    "  # devices.\n",
    "  pmap.assert_is_replicated(training_state)\n",
    "  params = _unpmap(\n",
    "      (training_state.normalizer_params, training_state.params.policy))\n",
    "  auto_params = _unpmap(\n",
    "      (training_state.auto_params.obs_encoder, training_state.auto_params.obs_decoder, training_state.auto_params.net_encoder, training_state.auto_params.net_decoder))\n",
    "  logging.info('total steps: %s', total_steps)\n",
    "  pmap.synchronize_hosts()\n",
    "  return (make_policy, params, auto_params, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "  \n",
    "  def get_default_rewards_config():\n",
    "    default_config = config_dict.ConfigDict(\n",
    "        dict(\n",
    "            # The coefficients for all reward terms used for training. All\n",
    "            # physical quantities are in SI units, if no otherwise specified,\n",
    "            # i.e. joint positions are in rad, positions are measured in meters,\n",
    "            # torques in Nm, and time in seconds, and forces in Newtons.\n",
    "            scales=config_dict.ConfigDict(\n",
    "                dict(\n",
    "                    # Tracking rewards are computed using exp(-delta^2/sigma)\n",
    "                    # sigma can be a hyperparameters to tune.\n",
    "                    # Track the base x-y velocity (no z-velocity tracking.)\n",
    "                    tracking_lin_vel=2,\n",
    "                    # Track the angular velocity along z-axis, i.e. yaw rate.\n",
    "                    tracking_ang_vel=0.8,\n",
    "                    # Below are regularization terms, we roughly divide the\n",
    "                    # terms to base state regularizations, joint\n",
    "                    # regularizations, and other behavior regularizations.\n",
    "                    # Penalize the base velocity in z direction, L2 penalty.\n",
    "                    lin_vel_z=-2.0,\n",
    "                    # Penalize the base roll and pitch rate. L2 penalty.\n",
    "                    ang_vel_xy=-0.05,\n",
    "                    # Penalize non-zero roll and pitch angles. L2 penalty.\n",
    "                    orientation=-2.0,\n",
    "                    # L2 regularization of joint torques, |tau|^2.\n",
    "                    torques=-0.0005,\n",
    "                    # Penalize the change in the action and encourage smooth\n",
    "                    # actions. L2 regularization |action - last_action|^2\n",
    "                    action_rate=-0.05,\n",
    "                    # Encourage long swing steps.  However, it does not\n",
    "                    # encourage high clearances.\n",
    "                    feet_air_time=0.5,\n",
    "                    # Encourage no motion at zero command, L2 regularization\n",
    "                    # |q - q_default|^2.\n",
    "                    stand_still=-1,\n",
    "                    # Early termination penalty.\n",
    "                    termination=-10.0,\n",
    "                    # Penalizing foot slipping on the ground.\n",
    "                    foot_slip=-0.1,\n",
    "                )\n",
    "            ),\n",
    "            # Tracking reward = exp(-error^2/sigma).\n",
    "            tracking_sigma=0.25,\n",
    "        )\n",
    "    )\n",
    "    return default_config\n",
    "\n",
    "  default_config = config_dict.ConfigDict(\n",
    "      dict(\n",
    "          rewards=get_default_rewards_config(),\n",
    "      )\n",
    "  )\n",
    "\n",
    "  return default_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GO1_Control(PipelineEnv):\n",
    "  \"\"\"Environment for training the quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = -0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = epath.Path('data/go1/go1_scene.xml') \n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004, 'dt': 0.004})\n",
    "\n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'trunk'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
    "    feet_site = [\n",
    "        'FL',\n",
    "        'RL',\n",
    "        'FR',\n",
    "        'RR',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'FL_calf',\n",
    "        'RL_calf',\n",
    "        'FR_calf',\n",
    "        'RR_calf',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.023\n",
    "    self._nv = sys.nv\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, key = jax.random.split(rng)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel  # pytype: disable=attribute-error\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    # motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    # done |= jp.any(joint_angles < self.lowers)\n",
    "    # done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),  # pytype: disable=attribute-error\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, -10000.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        state_info['command'] * jp.array([2.0, 2.0, 0.25]),  # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    # clip, noise\n",
    "    obs = jp.clip(obs, -100.0, 100.0) + self._obs_noise * jax.random.uniform(\n",
    "        state_info['rng'], obs.shape, minval=-1, maxval=1\n",
    "    )\n",
    "    \n",
    "    # stack observations through time\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize xy axes base angular velocity\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    # Penalize torques\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Penalize changes in actions\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of angular velocity commands (yaw)\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Reward air time.\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    # Penalize motion at zero commands\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # get velocities at feet which are offset from lower legs\n",
    "    # pytype: disable=attribute-error\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    # pytype: enable=attribute-error\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "  \n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GO1_Network(PipelineEnv):\n",
    "  \"\"\"Environment for training the quadruped joystick policy in MJX.\"\"\"\n",
    "\n",
    "  def __init__(\n",
    "      self,\n",
    "      obs_noise: float = 0.05,\n",
    "      action_scale: float = -0.3,\n",
    "      kick_vel: float = 0.05,\n",
    "      random_length: int = 2,\n",
    "      scale_multiplier: float = 1.0,\n",
    "      random_nets_on: bool = True,\n",
    "      **kwargs,\n",
    "  ):\n",
    "    path = epath.Path('data/go1/go1_scene.xml') \n",
    "    sys = mjcf.load(path.as_posix())\n",
    "    self._dt = 0.02  # this environment is 50 fps\n",
    "    sys = sys.tree_replace({'opt.timestep': 0.004, 'dt': 0.004})\n",
    "    \n",
    "    # override menagerie params for smoother policy\n",
    "    sys = sys.replace(\n",
    "        dof_damping=sys.dof_damping.at[6:].set(0.5239),\n",
    "        actuator_gainprm=sys.actuator_gainprm.at[:, 0].set(35.0),\n",
    "        actuator_biasprm=sys.actuator_biasprm.at[:, 1].set(-35.0),\n",
    "    )\n",
    "\n",
    "    n_frames = kwargs.pop('n_frames', int(self._dt / sys.opt.timestep))\n",
    "    super().__init__(sys, backend='mjx', n_frames=n_frames)\n",
    "    self.in_num = 12\n",
    "    self.out_num = 31\n",
    "    self.scale_multiplier = scale_multiplier\n",
    "    self.random_length = random_length\n",
    "    self.random_nets_on = random_nets_on\n",
    "    self.reward_config = get_config()\n",
    "    # set custom from kwargs\n",
    "    for k, v in kwargs.items():\n",
    "      if k.endswith('_scale'):\n",
    "        self.reward_config.rewards.scales[k[:-6]] = v\n",
    "\n",
    "    self._torso_idx = mujoco.mj_name2id(\n",
    "        sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, 'trunk'\n",
    "    )\n",
    "    self._action_scale = action_scale\n",
    "    self._obs_noise = obs_noise\n",
    "    self._kick_vel = kick_vel\n",
    "    self._init_q = jp.array(sys.mj_model.keyframe('home').qpos)\n",
    "    self._default_pose = sys.mj_model.keyframe('home').qpos[7:]\n",
    "    self.lowers = jp.array([-0.7, -1.0, 0.05] * 4)\n",
    "    self.uppers = jp.array([0.52, 2.1, 2.1] * 4)\n",
    "    feet_site = [\n",
    "        'FL',\n",
    "        'RL',\n",
    "        'FR',\n",
    "        'RR',\n",
    "    ]\n",
    "    feet_site_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_SITE.value, f)\n",
    "        for f in feet_site\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in feet_site_id), 'Site not found.'\n",
    "    self._feet_site_id = np.array(feet_site_id)\n",
    "    lower_leg_body = [\n",
    "        'FL_calf',\n",
    "        'RL_calf',\n",
    "        'FR_calf',\n",
    "        'RR_calf',\n",
    "    ]\n",
    "    lower_leg_body_id = [\n",
    "        mujoco.mj_name2id(sys.mj_model, mujoco.mjtObj.mjOBJ_BODY.value, l)\n",
    "        for l in lower_leg_body\n",
    "    ]\n",
    "    assert not any(id_ == -1 for id_ in lower_leg_body_id), 'Body not found.'\n",
    "    self._lower_leg_body_id = np.array(lower_leg_body_id)\n",
    "    self._foot_radius = 0.023\n",
    "    self._nv = sys.nv\n",
    "\n",
    "    self.hidden_one = tuple([self.in_num for _ in range(self.random_length)])\n",
    "    self.hidden_two = tuple([self.out_num for _ in range(self.random_length)])\n",
    "\n",
    "    self.normalize = running_statistics.normalize\n",
    "    self.normParams = running_statistics.init_state(specs.Array(self.out_num, jp.dtype('float32')))\n",
    "\n",
    "  def sample_command(self, rng: jax.Array) -> jax.Array:\n",
    "    lin_vel_x = [-0.6, 1.5]  # min max [m/s]\n",
    "    lin_vel_y = [-0.8, 0.8]  # min max [m/s]\n",
    "    ang_vel_yaw = [-0.7, 0.7]  # min max [rad/s]\n",
    "\n",
    "    _, key1, key2, key3 = jax.random.split(rng, 4)\n",
    "    lin_vel_x = jax.random.uniform(\n",
    "        key1, (1,), minval=lin_vel_x[0], maxval=lin_vel_x[1]\n",
    "    )\n",
    "    lin_vel_y = jax.random.uniform(\n",
    "        key2, (1,), minval=lin_vel_y[0], maxval=lin_vel_y[1]\n",
    "    )\n",
    "    ang_vel_yaw = jax.random.uniform(\n",
    "        key3, (1,), minval=ang_vel_yaw[0], maxval=ang_vel_yaw[1]\n",
    "    )\n",
    "    new_cmd = jp.array([lin_vel_x[0], lin_vel_y[0], ang_vel_yaw[0]])\n",
    "    return new_cmd\n",
    "\n",
    "  def reset(self, rng: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, key = jax.random.split(rng)\n",
    "    key, key1, key2, key3 = jax.random.split(key, 4)\n",
    "\n",
    "    scale = self.scale_multiplier*10**(-self.random_length + jax.random.uniform(key1, minval=0, maxval=self.random_length))\n",
    "\n",
    "    if self.random_nets_on:\n",
    "      random_network1 = make_random_network(self.in_num, self.hidden_one, scale=scale)\n",
    "      random_network2 = make_random_network(self.out_num, self.hidden_two, scale=scale)\n",
    "\n",
    "      p1 = random_network1.init(key2)\n",
    "      p2 = random_network2.init(key3)\n",
    "\n",
    "    pipeline_state = self.pipeline_init(self._init_q, jp.zeros(self._nv))\n",
    "\n",
    "    state_info = {\n",
    "        'rng': rng,\n",
    "        'last_act': jp.zeros(12),\n",
    "        'last_vel': jp.zeros(12),\n",
    "        'command': self.sample_command(key),\n",
    "        'last_contact': jp.zeros(4, dtype=bool),\n",
    "        'feet_air_time': jp.zeros(4),\n",
    "        'rewards': {k: 0.0 for k in self.reward_config.rewards.scales.keys()},\n",
    "        'kick': jp.array([0.0, 0.0]),\n",
    "        'step': 0,\n",
    "        'scale': scale if self.random_nets_on else 0,\n",
    "        'p1': p1 if self.random_nets_on else 0,\n",
    "        'p2': p2 if self.random_nets_on else 0, \n",
    "    }\n",
    "\n",
    "    obs_history = jp.zeros(15 * 31)  # store 15 steps of history\n",
    "    obs = self._get_obs(pipeline_state, state_info, obs_history)\n",
    "    reward, done = jp.zeros(2)\n",
    "    metrics = {'total_dist': 0.0}\n",
    "    for k in state_info['rewards']:\n",
    "      metrics[k] = state_info['rewards'][k]\n",
    "    state = State(pipeline_state, obs, reward, done, metrics, state_info)  # pytype: disable=wrong-arg-types\n",
    "    return state\n",
    "\n",
    "  def step(self, state: State, action: jax.Array) -> State:  # pytype: disable=signature-mismatch\n",
    "    rng, cmd_rng, kick_noise_2 = jax.random.split(state.info['rng'], 3)\n",
    "\n",
    "    if self.random_nets_on:\n",
    "      random_network1 = make_random_network(self.in_num, self.hidden_one, scale=state.info['scale'])\n",
    "\n",
    "      action = random_network1.apply(state.info['p1'], action)\n",
    "\n",
    "    # kick\n",
    "    push_interval = 10\n",
    "    kick_theta = jax.random.uniform(kick_noise_2, maxval=2 * jp.pi)\n",
    "    kick = jp.array([jp.cos(kick_theta), jp.sin(kick_theta)])\n",
    "    kick *= jp.mod(state.info['step'], push_interval) == 0\n",
    "    qvel = state.pipeline_state.qvel  # pytype: disable=attribute-error\n",
    "    qvel = qvel.at[:2].set(kick * self._kick_vel + qvel[:2])\n",
    "    state = state.tree_replace({'pipeline_state.qvel': qvel})\n",
    "\n",
    "    # physics step\n",
    "    motor_targets = self._default_pose + action * self._action_scale\n",
    "    # motor_targets = jp.clip(motor_targets, self.lowers, self.uppers)\n",
    "    pipeline_state = self.pipeline_step(state.pipeline_state, motor_targets)\n",
    "    x, xd = pipeline_state.x, pipeline_state.xd\n",
    "\n",
    "    # observation data\n",
    "    obs = self._get_obs(pipeline_state, state.info, state.obs)\n",
    "    joint_angles = pipeline_state.q[7:]\n",
    "    joint_vel = pipeline_state.qd[6:]\n",
    "\n",
    "    # foot contact data based on z-position\n",
    "    foot_pos = pipeline_state.site_xpos[self._feet_site_id]  # pytype: disable=attribute-error\n",
    "    foot_contact_z = foot_pos[:, 2] - self._foot_radius\n",
    "    contact = foot_contact_z < 1e-3  # a mm or less off the floor\n",
    "    contact_filt_mm = contact | state.info['last_contact']\n",
    "    contact_filt_cm = (foot_contact_z < 3e-2) | state.info['last_contact']\n",
    "    first_contact = (state.info['feet_air_time'] > 0) * contact_filt_mm\n",
    "    state.info['feet_air_time'] += self.dt\n",
    "\n",
    "    # done if joint limits are reached or robot is falling\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    done = jp.dot(math.rotate(up, x.rot[self._torso_idx - 1]), up) < 0\n",
    "    # done |= jp.any(joint_angles < self.lowers)\n",
    "    # done |= jp.any(joint_angles > self.uppers)\n",
    "    done |= pipeline_state.x.pos[self._torso_idx - 1, 2] < 0.18\n",
    "\n",
    "    # reward\n",
    "    rewards = {\n",
    "        'tracking_lin_vel': (\n",
    "            self._reward_tracking_lin_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'tracking_ang_vel': (\n",
    "            self._reward_tracking_ang_vel(state.info['command'], x, xd)\n",
    "        ),\n",
    "        'lin_vel_z': self._reward_lin_vel_z(xd),\n",
    "        'ang_vel_xy': self._reward_ang_vel_xy(xd),\n",
    "        'orientation': self._reward_orientation(x),\n",
    "        'torques': self._reward_torques(pipeline_state.qfrc_actuator),  # pytype: disable=attribute-error\n",
    "        'action_rate': self._reward_action_rate(action, state.info['last_act']),\n",
    "        'stand_still': self._reward_stand_still(\n",
    "            state.info['command'], joint_angles,\n",
    "        ),\n",
    "        'feet_air_time': self._reward_feet_air_time(\n",
    "            state.info['feet_air_time'],\n",
    "            first_contact,\n",
    "            state.info['command'],\n",
    "        ),\n",
    "        'foot_slip': self._reward_foot_slip(pipeline_state, contact_filt_cm),\n",
    "        'termination': self._reward_termination(done, state.info['step']),\n",
    "    }\n",
    "    rewards = {\n",
    "        k: v * self.reward_config.rewards.scales[k] for k, v in rewards.items()\n",
    "    }\n",
    "    reward = jp.clip(sum(rewards.values()) * self.dt, -10000.0, 10000.0)\n",
    "\n",
    "    # state management\n",
    "    state.info['kick'] = kick\n",
    "    state.info['last_act'] = action\n",
    "    state.info['last_vel'] = joint_vel\n",
    "    state.info['feet_air_time'] *= ~contact_filt_mm\n",
    "    state.info['last_contact'] = contact\n",
    "    state.info['rewards'] = rewards\n",
    "    state.info['step'] += 1\n",
    "    state.info['rng'] = rng\n",
    "\n",
    "    # sample new command if more than 500 timesteps achieved\n",
    "    state.info['command'] = jp.where(\n",
    "        state.info['step'] > 500,\n",
    "        self.sample_command(cmd_rng),\n",
    "        state.info['command'],\n",
    "    )\n",
    "    # reset the step counter when done\n",
    "    state.info['step'] = jp.where(\n",
    "        done | (state.info['step'] > 500), 0, state.info['step']\n",
    "    )\n",
    "\n",
    "    # log total displacement as a proxy metric\n",
    "    state.metrics['total_dist'] = math.normalize(x.pos[self._torso_idx - 1])[1]\n",
    "    state.metrics.update(state.info['rewards'])\n",
    "\n",
    "    done = jp.float32(done)\n",
    "\n",
    "    state = state.replace(\n",
    "        pipeline_state=pipeline_state, obs=obs, reward=reward, done=done\n",
    "    )\n",
    "    return state\n",
    "\n",
    "  def _get_obs(\n",
    "      self,\n",
    "      pipeline_state: base.State,\n",
    "      state_info: dict[str, Any],\n",
    "      obs_history: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    inv_torso_rot = math.quat_inv(pipeline_state.x.rot[0])\n",
    "    local_rpyrate = math.rotate(pipeline_state.xd.ang[0], inv_torso_rot)\n",
    "\n",
    "    if  self.random_nets_on:\n",
    "      random_network2 = make_random_network(self.out_num, self.hidden_two, scale=state_info['scale'])\n",
    "    \n",
    "    normal_command = state_info['command'] * jp.array([2.0, 2.0, 0.25])\n",
    "\n",
    "    obs = jp.concatenate([\n",
    "        jp.array([local_rpyrate[2]]) * 0.25,                 # yaw rate\n",
    "        math.rotate(jp.array([0, 0, -1]), inv_torso_rot),    # projected gravity\n",
    "        normal_command,                                      # command\n",
    "        pipeline_state.q[7:] - self._default_pose,           # motor angles\n",
    "        state_info['last_act'],                              # last action\n",
    "    ])\n",
    "\n",
    "    obs = jp.clip(obs, -100.0, 100.0)\n",
    "\n",
    "    obs = self.normalize(obs, self.normParams)\n",
    "\n",
    "    if self.random_nets_on:\n",
    "      obs = random_network2.apply(state_info['p2'], obs)\n",
    "      \n",
    "    # new_command = normal_command + self._obs_noise * jax.random.uniform(\n",
    "    #   state_info['rng'], (3,), minval=-1, maxval=1)\n",
    "    \n",
    "    # new_norm = jp.linalg.norm(new_command)\n",
    "\n",
    "    # new_command = new_command/new_norm\n",
    "\n",
    "    # obs = obs.at[4:7].set(new_command)\n",
    "\n",
    "\n",
    "    obs = jp.roll(obs_history, obs.size).at[:obs.size].set(obs)\n",
    "\n",
    "    return obs\n",
    "\n",
    "  # ------------ reward functions----------------\n",
    "  def _reward_lin_vel_z(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize z axis base linear velocity\n",
    "    return jp.square(xd.vel[0, 2])\n",
    "\n",
    "  def _reward_ang_vel_xy(self, xd: Motion) -> jax.Array:\n",
    "    # Penalize xy axes base angular velocity\n",
    "    return jp.sum(jp.square(xd.ang[0, :2]))\n",
    "\n",
    "  def _reward_orientation(self, x: Transform) -> jax.Array:\n",
    "    # Penalize non flat base orientation\n",
    "    up = jp.array([0.0, 0.0, 1.0])\n",
    "    rot_up = math.rotate(up, x.rot[0])\n",
    "    return jp.sum(jp.square(rot_up[:2]))\n",
    "\n",
    "  def _reward_torques(self, torques: jax.Array) -> jax.Array:\n",
    "    # Penalize torques\n",
    "    return jp.sqrt(jp.sum(jp.square(torques))) + jp.sum(jp.abs(torques))\n",
    "\n",
    "  def _reward_action_rate(\n",
    "      self, act: jax.Array, last_act: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Penalize changes in actions\n",
    "    return jp.sum(jp.square(act - last_act))\n",
    "\n",
    "  def _reward_tracking_lin_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of linear velocity commands (xy axes)\n",
    "    local_vel = math.rotate(xd.vel[0], math.quat_inv(x.rot[0]))\n",
    "    lin_vel_error = jp.sum(jp.square(commands[:2] - local_vel[:2]))\n",
    "    lin_vel_reward = jp.exp(\n",
    "        -lin_vel_error / self.reward_config.rewards.tracking_sigma\n",
    "    )\n",
    "    return lin_vel_reward\n",
    "\n",
    "  def _reward_tracking_ang_vel(\n",
    "      self, commands: jax.Array, x: Transform, xd: Motion\n",
    "  ) -> jax.Array:\n",
    "    # Tracking of angular velocity commands (yaw)\n",
    "    base_ang_vel = math.rotate(xd.ang[0], math.quat_inv(x.rot[0]))\n",
    "    ang_vel_error = jp.square(commands[2] - base_ang_vel[2])\n",
    "    return jp.exp(-ang_vel_error / self.reward_config.rewards.tracking_sigma)\n",
    "\n",
    "  def _reward_feet_air_time(\n",
    "      self, air_time: jax.Array, first_contact: jax.Array, commands: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # Reward air time.\n",
    "    rew_air_time = jp.sum((air_time - 0.1) * first_contact)\n",
    "    rew_air_time *= (\n",
    "        math.normalize(commands[:2])[1] > 0.05\n",
    "    )  # no reward for zero command\n",
    "    return rew_air_time\n",
    "\n",
    "  def _reward_stand_still(\n",
    "      self,\n",
    "      commands: jax.Array,\n",
    "      joint_angles: jax.Array,\n",
    "  ) -> jax.Array:\n",
    "    # Penalize motion at zero commands\n",
    "    return jp.sum(jp.abs(joint_angles - self._default_pose)) * (\n",
    "        math.normalize(commands[:2])[1] < 0.1\n",
    "    )\n",
    "\n",
    "  def _reward_foot_slip(\n",
    "      self, pipeline_state: base.State, contact_filt: jax.Array\n",
    "  ) -> jax.Array:\n",
    "    # get velocities at feet which are offset from lower legs\n",
    "    # pytype: disable=attribute-error\n",
    "    pos = pipeline_state.site_xpos[self._feet_site_id]  # feet position\n",
    "    feet_offset = pos - pipeline_state.xpos[self._lower_leg_body_id]\n",
    "    # pytype: enable=attribute-error\n",
    "    offset = base.Transform.create(pos=feet_offset)\n",
    "    foot_indices = self._lower_leg_body_id - 1  # we got rid of the world body\n",
    "    foot_vel = offset.vmap().do(pipeline_state.xd.take(foot_indices)).vel\n",
    "\n",
    "    # Penalize large feet velocity for feet that are in contact with the ground.\n",
    "    return jp.sum(jp.square(foot_vel[:, :2]) * contact_filt.reshape((-1, 1)))\n",
    "  \n",
    "  def _reward_termination(self, done: jax.Array, step: jax.Array) -> jax.Array:\n",
    "    return done & (step < 500)\n",
    "\n",
    "  def render(\n",
    "      self, trajectory: List[base.State], camera: str | None = None\n",
    "  ) -> Sequence[np.ndarray]:\n",
    "    camera = camera or 'track'\n",
    "    return super().render(trajectory, camera=camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs.register_environment('go1_control', GO1_Control)\n",
    "envs.register_environment('go1_network', GO1_Network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'go1_network'\n",
    "RANDOM_NETS_ON = True\n",
    "RANDOM_LENGTH = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = []\n",
    "y_data = []\n",
    "ydataerr = []\n",
    "times = [datetime.now()]\n",
    "html_vids = []\n",
    "c = [0]\n",
    "\n",
    "def display_vid(steps,make_fn,params,x=1,y=0,a=-0.5, save = True):\n",
    "  c[0] += 1\n",
    "  if c[0] % 100 != 0:\n",
    "    return\n",
    "\n",
    "  eval_env_vid = envs.get_environment(ENV_NAME, random_length=RANDOM_LENGTH, scale_multiplier=0, random_nets_on=False)\n",
    "  jit_reset = jax.jit(eval_env_vid.reset)\n",
    "  jit_step = jax.jit(eval_env_vid.step)\n",
    "\n",
    "  inference_fn = make_fn(params)\n",
    "  jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "  x_vel = x\n",
    "  y_vel = y\n",
    "  ang_vel = a\n",
    "\n",
    "  the_command = jp.array([x_vel, y_vel, ang_vel])\n",
    "\n",
    "  # initialize the state\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "  state = jit_reset(rng)\n",
    "  state.info['command'] = the_command\n",
    "  rollout = [state.pipeline_state]\n",
    "\n",
    "  # grab a trajectory\n",
    "  n_steps = 500\n",
    "  render_every = 2\n",
    "\n",
    "  for i in range(n_steps):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "    rollout.append(state.pipeline_state)\n",
    "\n",
    "    if state.done:\n",
    "      break\n",
    "  \n",
    "  if save:\n",
    "    model.save_params(f'policies/checkpoints/{ENV_NAME}_{RANDOM_NETS_ON}_{RANDOM_LENGTH}_{steps}', params)\n",
    "  \n",
    "  html_vids.append(media.show_video(eval_env_vid.render(rollout[::render_every], camera='tracking'),title=steps,return_html=True,fps=1.0 / eval_env_vid.dt / render_every))\n",
    "\n",
    "def progress(num_steps, metrics):\n",
    "  times.append(datetime.now())\n",
    "  x_data.append(num_steps)\n",
    "  y_data.append(metrics['eval/episode_reward'])\n",
    "  ydataerr.append(metrics['eval/episode_reward_std'])\n",
    "\n",
    "  max_y = max(10,max(y_data)*1.2)\n",
    "  min_y = min(0,min(y_data)*1.2)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  plt.figure(figsize=(10, 5))\n",
    "  plt.xlim([0, max(1000000,(num_steps+.0001) * 1.2)])\n",
    "  plt.ylim([min_y, max_y])\n",
    "  plt.xlabel('# environment steps')\n",
    "  plt.ylabel('reward per episode')\n",
    "  plt.title(f'Reward per Episode')\n",
    "\n",
    "  plt.errorbar(x_data, y_data, yerr=ydataerr, fmt='-o')\n",
    "\n",
    "  display(plt.gcf())\n",
    "  plt.close()\n",
    "\n",
    "  current_square = 2\n",
    "\n",
    "  if len(html_vids) > current_square**2:\n",
    "    html_vids.pop(0)\n",
    "\n",
    "  if current_square > 0:\n",
    "    grid_html = \"<table>\"\n",
    "    for i in range(0, current_square**2, current_square):\n",
    "        grid_html += \"<tr>\"\n",
    "        for j in range(current_square):\n",
    "            if i+j >= len(html_vids):\n",
    "                grid_html += f\"<td></td>\"\n",
    "                break\n",
    "            grid_html += f\"<td>{html_vids[i+j]}</td>\"\n",
    "        grid_html += \"</tr>\"\n",
    "    grid_html += \"</table>\"\n",
    "    display(HTML(grid_html))\n",
    "\n",
    "def domain_randomize(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree.map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes\n",
    "\n",
    "def domain_randomize_net(sys, rng):\n",
    "  \"\"\"Randomizes the mjx.Model.\"\"\"\n",
    "  @jax.vmap\n",
    "  def rand(rng):\n",
    "    _, key = jax.random.split(rng, 2)\n",
    "    # friction\n",
    "    friction = jax.random.uniform(key, (1,), minval=0.6, maxval=1.4)\n",
    "    friction = sys.geom_friction.at[:, 0].set(friction)\n",
    "    # actuator\n",
    "    _, key = jax.random.split(key, 2)\n",
    "    gain_range = (-5, 5)\n",
    "    param = jax.random.uniform(\n",
    "        key, (1,), minval=gain_range[0], maxval=gain_range[1]\n",
    "    ) + sys.actuator_gainprm[:, 0]\n",
    "    gain = sys.actuator_gainprm.at[:, 0].set(param)\n",
    "    bias = sys.actuator_biasprm.at[:, 1].set(-param)\n",
    "    return friction, gain, bias\n",
    "\n",
    "  friction, gain, bias = rand(rng)\n",
    "\n",
    "  in_axes = jax.tree.map(lambda x: None, sys)\n",
    "  in_axes = in_axes.tree_replace({\n",
    "      'geom_friction': 0,\n",
    "      'actuator_gainprm': 0,\n",
    "      'actuator_biasprm': 0,\n",
    "  })\n",
    "\n",
    "  sys = sys.tree_replace({\n",
    "      'geom_friction': friction,\n",
    "      'actuator_gainprm': gain,\n",
    "      'actuator_biasprm': bias,\n",
    "  })\n",
    "\n",
    "  return sys, in_axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from jax import make_jaxpr\n",
    "\n",
    "# eval_env_v = envs.get_environment(ENV_NAME)\n",
    "# jit_reset = jax.jit(eval_env_v.reset)\n",
    "# jit_step = jax.jit(eval_env_v.step)\n",
    "\n",
    "# s = jit_reset(jax.random.PRNGKey(0))\n",
    "\n",
    "# print(make_jaxpr(jit_step)(s,jp.array([0]*12)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vvv = 5\n",
    "# p = 0\n",
    "# aaa = 300\n",
    "# hid = (vvv,vvv,vvv,vvv,vvv,vvv,vvv,vvv)\n",
    "# for i in range(aaa):\n",
    "\n",
    "#     random_network1 = make_random_network(vvv, hid, scale=10**(-len(hid)+np.random.uniform(0,len(hid))))\n",
    "\n",
    "#     p1 = random_network1.init(jax.random.PRNGKey(i))\n",
    "\n",
    "#     out = random_network1.apply(p1, jp.array([1]*vvv))\n",
    "#     d = jp.dot(out-jp.array([1]*vvv),out-jp.array([1]*vvv))**2\n",
    "#     if d < .1:\n",
    "#         p +=1\n",
    "\n",
    "# print(p/aaa * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_networks_factory = functools.partial(\n",
    "    ppo_networks.make_ppo_networks,\n",
    "        policy_hidden_layer_sizes=(2*128, 2*128, 2*128, 2*128))\n",
    "make_networks_factory_autoencode = functools.partial(\n",
    "    make_encode_decode,\n",
    "        squash_size=64,hidden_layer_sizes_encode=(256,128),hidden_layer_sizes_decode=(128,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_fn = functools.partial(\n",
    "      train, num_timesteps=500_000_000, num_evals=405,\n",
    "      reward_scaling=1, episode_length=1000, normalize_observations=False,\n",
    "      action_repeat=1, unroll_length=20, num_minibatches=32,\n",
    "      num_updates_per_batch=4, discounting=0.97, learning_rate=1.0e-4,\n",
    "      entropy_cost=1e-2, num_envs=2**13, batch_size=256,\n",
    "      network_factory=make_networks_factory,\n",
    "      auto_encode_factory=make_networks_factory_autoencode,\n",
    "      seed=1,\n",
    "      progress_fn=progress,\n",
    "      policy_params_fn=display_vid)\n",
    "\n",
    "env = envs.get_environment(ENV_NAME, random_length=RANDOM_LENGTH, scale_multiplier=1, random_nets_on=RANDOM_NETS_ON)\n",
    "eval_env = envs.get_environment(ENV_NAME, random_length=RANDOM_LENGTH, scale_multiplier=0, random_nets_on=False)\n",
    "\n",
    "make_inference_fn, params, auto_params, _= train_fn(environment=env, eval_env=eval_env)\n",
    "\n",
    "print(f'time to jit: {times[1] - times[0]}')\n",
    "print(f'time to train: {times[-1] - times[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'policies/{ENV_NAME}_{RANDOM_NETS_ON}_{RANDOM_LENGTH}'\n",
    "auto_model_path = f'policies/auto_encoder/auto_encoder_{RANDOM_NETS_ON}_{RANDOM_LENGTH}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_params(model_path, params)\n",
    "model.save_params(auto_model_path, auto_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn_q = functools.partial(\n",
    "      train, num_timesteps=1, num_evals=1, episode_length=1, normalize_observations=False, num_envs=1, batch_size=1,\n",
    "      network_factory=make_networks_factory, auto_encode_factory=make_networks_factory_autoencode)\n",
    "\n",
    "e = envs.get_environment(ENV_NAME, random_length=RANDOM_LENGTH, scale_multiplier=0, random_nets_on=False)\n",
    "make_inference_fn,_, _, _= train_fn_q(environment=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.load_params(model_path)\n",
    "auto_params = model.load_params(auto_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_data(make_fn,params,auto_fact,auto_params,n_steps=500,start_flat=None,random_nets=False,scale=1,command=[1,0,-0.5]):\n",
    "\n",
    "  eval_env_vid = envs.get_environment(ENV_NAME, random_length=RANDOM_LENGTH, scale_multiplier=scale, random_nets_on=random_nets)\n",
    "  jit_reset = jax.jit(eval_env_vid.reset)\n",
    "  jit_step = jax.jit(eval_env_vid.step)\n",
    "\n",
    "  inference_fn = make_fn(params)\n",
    "  jit_inference_fn = jax.jit(inference_fn)\n",
    "\n",
    "  # initialize the state\n",
    "  rng = jax.random.PRNGKey(0)\n",
    "  state = jit_reset(rng)\n",
    "  state.info['command'] = jp.array(command)\n",
    "  if start_flat != None:\n",
    "    p1,p2 = inverse_flatten_single(start_flat,12,31,RANDOM_LENGTH, RANDOM_LENGTH)\n",
    "    state.info['p1']['params'] = p1\n",
    "    state.info['p2']['params'] = p2\n",
    "  rollout = [state.pipeline_state]\n",
    "\n",
    "  if random_nets:\n",
    "    flat = flatten_auto_encoder(state.info['p1']['params'],state.info['p2']['params'])\n",
    "    pcount = flat.shape[-1]\n",
    "    obs_encoder, obs_decoder = auto_fact(\n",
    "      data_size_encode = state.obs.shape[-1],\n",
    "      data_size_decode = state.obs.shape[-1])\n",
    "\n",
    "    net_encoder, net_decoder = auto_fact(\n",
    "    data_size_encode = pcount,\n",
    "    data_size_decode =pcount)\n",
    "\n",
    "    flat_encode = net_encoder.apply(auto_params[2], flat)\n",
    "\n",
    "  embeds = []\n",
    "\n",
    "  for i in range(n_steps):\n",
    "    act_rng, rng = jax.random.split(rng)\n",
    "    if random_nets:\n",
    "      obs_embed = obs_encoder.apply(auto_params[0], state.obs)\n",
    "      embeds.append(obs_embed)\n",
    "    ctrl, _ = jit_inference_fn(state.obs, act_rng)\n",
    "    state = jit_step(state, ctrl)\n",
    "\n",
    "    rollout.append(state.pipeline_state)\n",
    "\n",
    "    if state.done:\n",
    "      break\n",
    "  \n",
    "  if random_nets:\n",
    "    av_embedds = jp.average(jp.array(embeds),axis=0)\n",
    "    flat_decoded = net_decoder.apply(auto_params[3], av_embedds)\n",
    "\n",
    "\n",
    "  return flat, flat_encode, flat_decoded, av_embedds, embeds, rollout, eval_env_vid\n",
    "\n",
    "def renderGrid(amount,a=(-.5,.5),x=(-.5,.5),y=(-.5,.5)):\n",
    "  isa = a[0] < a[1]\n",
    "  isx = x[0] < x[1]\n",
    "  isy = y[0] < y[1]\n",
    "\n",
    "  for i in range(amount):\n",
    "    for j in range(amount):\n",
    "      if isa and isx:\n",
    "        _a = (a[1]-a[0])*(i/(amount-1)) + a[0]\n",
    "        _x = (x[1]-x[0])*(j/(amount-1)) + x[0]\n",
    "        _y = y[0]\n",
    "      elif isa and isy:\n",
    "        _y = (y[1]-y[0])*(i/(amount-1)) + y[0]\n",
    "        _a = (a[1]-a[0])*(j/(amount-1)) + a[0]\n",
    "        _x = x[0]\n",
    "      elif isx and isy:\n",
    "        _y = (y[1]-y[0])*(i/(amount-1)) + y[0]\n",
    "        _x = (x[1]-x[0])*(j/(amount-1)) + x[0]\n",
    "        _a = a[0]\n",
    "      else:\n",
    "        raise ValueError('One of the axis must be a range')\n",
    "      \n",
    "      display_vid(f\"a = {_a:.2f} x = {_x:.2f} y = {_y:.2f}\",make_inference_fn,params,a=_a,x=_x,y=_y, save=False)\n",
    "\n",
    "      clear_output(wait=True)\n",
    "      grid_html = \"<table>\"\n",
    "      for ii in range(0, amount**2, amount):\n",
    "          grid_html += \"<tr>\"\n",
    "          for jj in range(amount):\n",
    "              if ii+jj >= len(html_vids):\n",
    "                grid_html += f\"<td></td>\"\n",
    "                break\n",
    "              grid_html += f\"<td>{html_vids[ii+jj]}</td>\"\n",
    "          grid_html += \"</tr>\"\n",
    "      grid_html += \"</table>\"\n",
    "      display(HTML(grid_html))\n",
    "\n",
    "  return grid_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rend_vid(lable,rollout,eval_env_vid):\n",
    "    render_every = 2\n",
    "    media.show_video(eval_env_vid.render(rollout[::render_every], camera='tracking'),title=lable,fps=1.0 / eval_env_vid.dt / render_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat, flat_encode, flat_decoded, av_embedds, embeds, rollout, env = gather_data(make_inference_fn,params,make_networks_factory_autoencode,auto_params,n_steps=500,random_nets=True,scale=0,command=[1,0,-0.5])\n",
    "\n",
    "flat2, flat_encode2, flat_decoded2, av_embedds2, embeds2, rollout2, env2 = gather_data(make_inference_fn,params,make_networks_factory_autoencode,auto_params,start_flat=flat_decoded,n_steps=500,random_nets=True,scale=10,command=[1,0,-0.5])\n",
    "\n",
    "rend_vid('true',rollout,env)\n",
    "rend_vid('decode',rollout2,env2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html_vids = []\n",
    "\n",
    "# xy_viz = renderGrid(3,a=(-.5,.5),x=(-.5,1),y=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
