---
  agent_network:
    name: 'default'
    head_params:
      hidden_layer_sizes:
        - 32
      activation: 'relu'
      activate_final: False
    ppo_params:
      value_params:
        hidden_layer_sizes:
          - 32
        activation: 'relu'
        activate_final: False
      policy_params:
        hidden_layer_sizes:
          - 32
        activation: 'relu'
        activate_final: False
  env_network:
    name: 'default'
    in_params:
      hidden_layer_sizes:
        - 32
      activation: 'tanh'
      activate_final: True
    out_params:
      hidden_layer_sizes:
        - 32
      activation: 'tanh'
      activate_final: True
  agent_train:
    name: 'default'
    num_timesteps: 11000
    episode_length: 100
    action_repeat: 1
    num_envs: 4
    num_eval_envs: 10
    num_evals: 3
    learning_rate: 0.0003
    entropy_cost: 0.0001
    discounting: 0.9
    seed: 0
    unroll_length: 10
    batch_size: 16
    num_minibatches: 32
    num_updates_per_batch: 4
    num_resets_per_eval: 0
    normalize_observations: True
    normalize_advantage: True
    reward_scaling: 1.0
    clipping_epsilon: 0.3
    gae_lambda: 0.95
  env_train:
    name: 'default'
    learning_rate: 0.0003
    seed: 0
    unroll_length: 20
    type_size: 4
    type_split_every: 10
    data_loops: 10
    num_minibatches: 1
    normalize_observations: True
  enviroment:
    name: 'joy_stick_env'
    enviroment_params:
      action_scale: -0.3
      kick_vel: 0.05
      reward_config:
        tracking_sigma: 0.25
        scales:
          tracking_lin_vel: 2
          tracking_ang_vel: 0.8
          lin_vel_z: -2.0
          ang_vel_xy: -0.05
          orientation: 0.0
          torques: -0.0005
          action_rate: -0.05
          feet_air_time: 0.5
          stand_still: -1
          termination: -10.0
          foot_slip: -0.1

    
