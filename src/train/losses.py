from typing import Any, Tuple
from typing import Sequence, Callable
from brax.training.types import Params, PRNGKey, NamedTuple
import flax
import jax
import jax.numpy as jp
from brax import envs
from brax.training.acme import running_statistics
from networks.ppo import ppo_network, ppo_network_params

from train.acting import unroll_policy

_PMAP_AXIS_NAME = 'i'

def compute_gae(truncation: jp.ndarray,
                termination: jp.ndarray,
                rewards: jp.ndarray,
                values: jp.ndarray,
                bootstrap_value: jp.ndarray,
                lambda_: float = 1.0,
                discount: float = 0.99):
  """Calculates the Generalized Advantage Estimation (GAE).

  Args:
    truncation: A float32 tensor of shape [T, B] with truncation signal.
    termination: A float32 tensor of shape [T, B] with termination signal.
    rewards: A float32 tensor of shape [T, B] containing rewards generated by
      following the behaviour policy.
    values: A float32 tensor of shape [T, B] with the value function estimates
      wrt. the target policy.
    bootstrap_value: A float32 of shape [B] with the value function estimate at
      time T.
    lambda_: Mix between 1-step (lambda_=0) and n-step (lambda_=1). Defaults to
      lambda_=1.
    discount: TD discount.

  Returns:
    A float32 tensor of shape [T, B]. Can be used as target to
      train a baseline (V(x_t) - vs_t)^2.
    A float32 tensor of shape [T, B] of advantages.
  """

  truncation_mask = 1 - truncation
  # Append bootstrapped value to get [v1, ..., v_t+1]
  values_t_plus_1 = jp.concatenate(
      [values[1:], jp.expand_dims(bootstrap_value, 0)], axis=0)
  deltas = rewards + discount * (1 - termination) * values_t_plus_1 - values
  deltas *= truncation_mask

  acc = jp.zeros_like(bootstrap_value)
  vs_minus_v_xs = []

  def compute_vs_minus_v_xs(carry, target_t):
    lambda_, acc = carry
    truncation_mask, delta, termination = target_t
    acc = delta + discount * (1 - termination) * truncation_mask * lambda_ * acc
    return (lambda_, acc), (acc)

  (_, _), (vs_minus_v_xs) = jax.lax.scan(
      compute_vs_minus_v_xs, (lambda_, acc),
      (truncation_mask, deltas, termination),
      length=int(truncation_mask.shape[0]),
      reverse=True)
  # Add V(x_s) to get v_s.
  vs = jp.add(vs_minus_v_xs, values)

  vs_t_plus_1 = jp.concatenate(
      [vs[1:], jp.expand_dims(bootstrap_value, 0)], axis=0)
  advantages = (rewards + discount *
                (1 - termination) * vs_t_plus_1 - values) * truncation_mask
  return jax.lax.stop_gradient(vs), jax.lax.stop_gradient(advantages)


def compute_ppo_loss(
    params: ppo_network_params,
    start_state: envs.State,
    normalizer_params: Any,
    rng: jp.ndarray,
    ppo_network: ppo_network,
    env: envs.Env,
    unroll_length: int = 20,
    entropy_cost: float = 1e-4,
    discounting: float = 0.9,
    reward_scaling: float = 1.0,
    gae_lambda: float = 0.95,
    clipping_epsilon: float = 0.3,
    normalize_advantage: bool = True) -> Tuple[jp.ndarray, Any]:

    key1, key2 = jax.random.split(rng)

    final_state, data = unroll_policy(ppo_network,normalizer_params,params,start_state,key1,env,unroll_length)

    normalizer_params = running_statistics.update(
        normalizer_params,
        data.observation,
        pmap_axis_name=_PMAP_AXIS_NAME)

    next_observation = ppo_network.normalizer(data.next_observation[-1], normalizer_params)

    if data.next_hidden_state is None:
      hidden, _ = ppo_network.head_network.apply(params.head, next_observation, None)
    else: 
      hidden, _ = ppo_network.head_network.apply(params.head, next_observation, data.next_hidden_state[-1])
    
    bootstrap_value, _ = ppo_network.value_network.apply(params.value, hidden, None)

    bootstrap_value = jp.squeeze(bootstrap_value, axis=-1)

    rewards = data.reward * reward_scaling
    truncation = data.truncation
    termination = (1 - data.discount) * (1 - truncation)

    target_action_log_probs = ppo_network.action_distribution.log_prob(
        data.logits, data.raw_actions)
    behaviour_action_log_probs = data.log_prob

    vs, advantages = compute_gae(
        truncation=truncation,
        termination=termination,
        rewards=rewards,
        values=data.baseline,
        bootstrap_value=bootstrap_value,
        lambda_=gae_lambda,
        discount=discounting)
    if normalize_advantage:
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
    rho_s = jp.exp(target_action_log_probs - behaviour_action_log_probs)

    surrogate_loss1 = rho_s * advantages
    surrogate_loss2 = jp.clip(rho_s, 1 - clipping_epsilon,
                                1 + clipping_epsilon) * advantages

    policy_loss = -jp.mean(jp.minimum(surrogate_loss1, surrogate_loss2))

    # Value function loss
    v_error = vs - data.baseline
    v_loss = jp.mean(v_error * v_error) * 0.5 * 0.5

    # Entropy reward
    entropy = jp.mean(ppo_network.action_distribution.entropy(data.logits, key2))
    entropy_loss = entropy_cost * -entropy

    total_loss = policy_loss + v_loss + entropy_loss
    return total_loss, {
       'loss_metrics':{
          'total_loss': total_loss,
          'policy_loss': policy_loss,
          'v_loss': v_loss,
          'entropy_loss': entropy_loss
        },
        'state_info':{
          'final_state': final_state 
        },
        'normalizer_params': normalizer_params
    }
